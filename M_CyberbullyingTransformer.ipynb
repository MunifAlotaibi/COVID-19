{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "M-CyberbullyingTransformer.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1dMjTnl5-ZUl62hwFXvdHmAdhufjCAXC2",
      "authorship_tag": "ABX9TyNKyaJEuvljz+ycw+Z6RbUZ"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NICArE1RMtG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef10c4b-1a35-43d7-e826-3ca1137eea39"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
        "from time import time\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import linear_model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.externals import joblib\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import SpatialDropout1D, concatenate\n",
        "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, LSTM\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.utils.vis_utils import plot_model\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "-nMWFeieCZbz",
        "outputId": "a775e040-245e-4e49-9c0d-c830782f82c8"
      },
      "source": [
        "from pandas.io.json import json_normalize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Dataset for Detection of Cyber-Trolls.json\n",
        "data3 = pd.read_json('Cyber-Trolls.json', lines= True,orient='columns')\n",
        "#fd = pd.read_json('/data1.json')\n",
        "\n",
        "for i in range(0,len(data3)):\n",
        "    if data3.annotation[i]['label'][0] == '1':\n",
        "        data3.annotation[i] = 1\n",
        "    else:\n",
        "        data3.annotation[i] = 0\n",
        "\n",
        "data3"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>annotation</th>\n",
              "      <th>extras</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Get fucking real dude.</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>She is as dirty as they come  and that crook ...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>why did you fuck it up. I could do it all day...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WTF are you talking about Men? No men thats n...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>I dont. But what is complaining about it goi...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>Bahah  yeah i&amp;;m totally just gonna&amp;; get pis...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>hahahahaha &gt;:) im evil mwahahahahahahahahaha</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>What&amp;;s something unique about Ohio? :)</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000</th>\n",
              "      <td>Who is the biggest gossiper you know?</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20001 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content annotation  extras\n",
              "0                                 Get fucking real dude.          1     NaN\n",
              "1       She is as dirty as they come  and that crook ...          1     NaN\n",
              "2       why did you fuck it up. I could do it all day...          1     NaN\n",
              "3       Dude they dont finish enclosing the fucking s...          1     NaN\n",
              "4       WTF are you talking about Men? No men thats n...          1     NaN\n",
              "...                                                  ...        ...     ...\n",
              "19996    I dont. But what is complaining about it goi...          0     NaN\n",
              "19997   Bahah  yeah i&;m totally just gonna&; get pis...          0     NaN\n",
              "19998       hahahahaha >:) im evil mwahahahahahahahahaha          0     NaN\n",
              "19999            What&;s something unique about Ohio? :)          0     NaN\n",
              "20000              Who is the biggest gossiper you know?          0     NaN\n",
              "\n",
              "[20001 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "3JdGPBApUsQp",
        "outputId": "37aaff02-3501-4671-f1e7-693eb2cbc447"
      },
      "source": [
        "data3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>annotation</th>\n",
              "      <th>extras</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Get fucking real dude.</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>She is as dirty as they come  and that crook ...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>why did you fuck it up. I could do it all day...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WTF are you talking about Men? No men thats n...</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>I dont. But what is complaining about it goi...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>Bahah  yeah i&amp;;m totally just gonna&amp;; get pis...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>hahahahaha &gt;:) im evil mwahahahahahahahahaha</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>What&amp;;s something unique about Ohio? :)</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000</th>\n",
              "      <td>Who is the biggest gossiper you know?</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20001 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content annotation  extras\n",
              "0                                 Get fucking real dude.          1     NaN\n",
              "1       She is as dirty as they come  and that crook ...          1     NaN\n",
              "2       why did you fuck it up. I could do it all day...          1     NaN\n",
              "3       Dude they dont finish enclosing the fucking s...          1     NaN\n",
              "4       WTF are you talking about Men? No men thats n...          1     NaN\n",
              "...                                                  ...        ...     ...\n",
              "19996    I dont. But what is complaining about it goi...          0     NaN\n",
              "19997   Bahah  yeah i&;m totally just gonna&; get pis...          0     NaN\n",
              "19998       hahahahaha >:) im evil mwahahahahahahahahaha          0     NaN\n",
              "19999            What&;s something unique about Ohio? :)          0     NaN\n",
              "20000              Who is the biggest gossiper you know?          0     NaN\n",
              "\n",
              "[20001 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft3V3FB6Mt39"
      },
      "source": [
        "data1 = pd.read_csv('labeled_tweets.csv')\n",
        "data2 = pd.read_csv('public_data_labeled.csv')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqtChQ_-MzIu"
      },
      "source": [
        "data1.drop_duplicates(inplace = True)\n",
        "data1.drop('id', axis = 'columns', inplace = True)\n",
        "data2.drop_duplicates(inplace = True)\n",
        "data3.drop('extras', axis = 'columns', inplace = True)\n",
        "#data3.drop_duplicates(inplace = True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmk-ilzryGbo",
        "outputId": "53667fe2-645e-4d7a-a328-f8b8e7f6c616"
      },
      "source": [
        "#import preprocessor as p\n",
        "!pip install tweet-preprocessor\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "4YG0uYxXy4pk",
        "outputId": "1d6b7f81-8f5b-403c-f048-79baaa845c8e"
      },
      "source": [
        "import preprocessor as p\n",
        "\n",
        "\n",
        "def preprocess_tweet(row):\n",
        "    full_text = row['full_text']\n",
        "    full_text = p.clean(full_text)\n",
        "    return full_text\n",
        "\n",
        "data1['full_text'] = data1.apply(preprocess_tweet, axis=1)\n",
        "data2['full_text'] = data2.apply(preprocess_tweet, axis=1)\n",
        "\n",
        "\n",
        "def preprocess_tweet3(row):\n",
        "    content = row['content']\n",
        "    content = p.clean(content)\n",
        "    return content\n",
        "\n",
        "data3['content'] = data3.apply(preprocess_tweet3, axis=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0af9f81395e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfull_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjwVh-vw1G4w"
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "def stopword_removal(row):\n",
        "    full_text = row['full_text']\n",
        "    full_text = remove_stopwords(full_text)\n",
        "    return full_text\n",
        "\n",
        "data1['full_text'] = data1.apply(stopword_removal, axis=1)\n",
        "data2['full_text'] = data2.apply(stopword_removal, axis=1)\n",
        "\n",
        "data1['full_text'] = data1['full_text'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')\n",
        "data2['full_text'] = data2['full_text'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')\n",
        "\n",
        "def stopword_removal3(row):\n",
        "    content = row['content']\n",
        "    content = remove_stopwords(content)\n",
        "    return content\n",
        "\n",
        "data3['content'] = data3.apply(stopword_removal3, axis=1)\n",
        "data3['content'] = data3['content'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "TuakqhRAzCH_",
        "outputId": "f525693d-be09-4505-c35a-b42fb6875795"
      },
      "source": [
        "data1.head(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>So Drasko just said he was impressed the girls...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>Drasko they didn't cook half a bird you idiot ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>Hopefully someone cooks Drasko in the next ep ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>of course you were born in serbia...you're as ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>These girls are the equivalent of the irritati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>Oh yeah Colin! Smash those girls! :D #MKR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>Kel the butcher has never heard of these silly...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>@mykitchenrules Did Stevie Wonder choose these...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>#MKR2015 #MKR why do they keep saying that the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>Offensive</td>\n",
              "      <td>Is #mkr sexist then? Four of the six teams in ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         label                                          full_text\n",
              "0    Offensive  So Drasko just said he was impressed the girls...\n",
              "2    Offensive  Drasko they didn't cook half a bird you idiot ...\n",
              "4    Offensive  Hopefully someone cooks Drasko in the next ep ...\n",
              "6    Offensive  of course you were born in serbia...you're as ...\n",
              "7    Offensive  These girls are the equivalent of the irritati...\n",
              "..         ...                                                ...\n",
              "124  Offensive          Oh yeah Colin! Smash those girls! :D #MKR\n",
              "125  Offensive  Kel the butcher has never heard of these silly...\n",
              "126  Offensive  @mykitchenrules Did Stevie Wonder choose these...\n",
              "127  Offensive  #MKR2015 #MKR why do they keep saying that the...\n",
              "129  Offensive  Is #mkr sexist then? Four of the six teams in ...\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45kplwUrM1Yh"
      },
      "source": [
        "data = pd.concat([data1, data2])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "dKmwoAUqM56O",
        "outputId": "8255885a-35b1-4ca6-c3cd-c6c714a7a40f"
      },
      "source": [
        "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
        "\n",
        "plt.figure(figsize = (7,7))\n",
        "sorted_counts = data['label'].value_counts()\n",
        "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 180, counterclock = False, wedgeprops = {'width' : 0.4},\n",
        "       autopct='%1.1f%%', pctdistance = 0.8, textprops = {'color': 'black', 'fontsize' : 13}, shadow = True,\n",
        "        colors = sns.color_palette(flatui))\n",
        "plt.text(x = -0.35, y = 0, s = 'Total Tweets: {}'.format(data.shape[0]))\n",
        "plt.title('Category of Tweets in the Dataset', fontsize = 16);\n",
        "dpi = 150\n",
        "#plt.savefig('tweetsfig.eps', dpi=dpi) # 3.8M!"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "wV1LqmFVm4b_",
        "outputId": "81ad5be6-2cc9-4a7d-c1f9-9f08b6e069e1"
      },
      "source": [
        "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
        "\n",
        "plt.figure(figsize = (7,7))\n",
        "sorted_counts = data3['annotation'].value_counts()\n",
        "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 180, counterclock = False, wedgeprops = {'width' : 0.4},\n",
        "       autopct='%1.1f%%', pctdistance = 0.8, textprops = {'color': 'black', 'fontsize' : 13}, shadow = True,\n",
        "        colors = sns.color_palette(flatui))\n",
        "plt.text(x = -0.35, y = 0, s = 'Total Tweets: {}'.format(data3.shape[0]))\n",
        "plt.title('Category of Tweets in the Dataset', fontsize = 16);\n",
        "dpi = 150\n",
        "#plt.savefig('tweetsfig.eps', dpi=dpi) # 3.8M!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCkQd9rnM8HZ"
      },
      "source": [
        "data['label'] = data.label.map({'Offensive': 1, 'Non-offensive': 0})"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26yILsYYHOCp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "QTZ4QbD1NBFH",
        "outputId": "aa2d748a-52a0-4be6-e173-19fbb63dc2e0"
      },
      "source": [
        "data3.columns = ['full_text', 'label']\r\n",
        "\r\n",
        "data3"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Get fucking real dude.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>She is as dirty as they come  and that crook ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>why did you fuck it up. I could do it all day...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WTF are you talking about Men? No men thats n...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>I dont. But what is complaining about it goi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>Bahah  yeah i&amp;;m totally just gonna&amp;; get pis...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>hahahahaha &gt;:) im evil mwahahahahahahahahaha</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>What&amp;;s something unique about Ohio? :)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000</th>\n",
              "      <td>Who is the biggest gossiper you know?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20001 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               full_text label\n",
              "0                                 Get fucking real dude.     1\n",
              "1       She is as dirty as they come  and that crook ...     1\n",
              "2       why did you fuck it up. I could do it all day...     1\n",
              "3       Dude they dont finish enclosing the fucking s...     1\n",
              "4       WTF are you talking about Men? No men thats n...     1\n",
              "...                                                  ...   ...\n",
              "19996    I dont. But what is complaining about it goi...     0\n",
              "19997   Bahah  yeah i&;m totally just gonna&; get pis...     0\n",
              "19998       hahahahaha >:) im evil mwahahahahahahahahaha     0\n",
              "19999            What&;s something unique about Ohio? :)     0\n",
              "20000              Who is the biggest gossiper you know?     0\n",
              "\n",
              "[20001 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVp86roGxwL1"
      },
      "source": [
        "data = pd.concat([data, data3])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nV0zHf7KCOR",
        "outputId": "b05cd654-8837-4762-d183-8c37119d2b3b"
      },
      "source": [
        "sorted_counts = data['label'].value_counts()\r\n",
        "sorted_counts"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    31353\n",
              "0    24435\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRkDEAqhBpi5",
        "outputId": "3b747a13-cdf6-488c-e18d-d620dc1ced94"
      },
      "source": [
        "sorted_counts"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    31353\n",
              "0    24435\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QikIOe74MplW",
        "outputId": "4e9e86a6-2349-4531-fc55-64c68d7b98f7"
      },
      "source": [
        "! pip install transformers\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 23.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 56.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=6b8e78e90d470d877c98821ca1540875f59c7b6c7a778e43a51f03a5f0c703e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG4qeWVaMKQB"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F7ksqxLO7O1"
      },
      "source": [
        "\n",
        "# fit model on dataset\n",
        "def fit_model(padded_train_sequences, Y_train):\n",
        "\t# define model\n",
        "  embed_dim = 32  # Embedding size for each token\n",
        "  num_heads = 2  # Number of attention heads\n",
        "  ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "  maxlen = 150\n",
        "  vocab_size = 80000\n",
        "  inputs = layers.Input(shape=(maxlen,))\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "  x = embedding_layer(inputs)\n",
        "  transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "  x = transformer_block(x)\n",
        "  x = layers.GlobalAveragePooling1D()(x)\n",
        "  x = layers.Dropout(0.1)(x)\n",
        "  x = layers.Dense(200, activation=\"relu\")(x)\n",
        "  x = layers.Dropout(0.1)(x)\n",
        "  outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  # fit model\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "  history = model.fit(x=padded_train_sequences, y=Y_train, \n",
        "                    #validation_data=(padded_test_sequences, Y_test), \n",
        "                    batch_size=batch_size, \n",
        "                    #callbacks=[checkpoint], \n",
        "                    epochs=8, \n",
        "                    verbose=1)\n",
        "  #accuracyscore = modelv.evaluate(padded_test_sequences, Y_test)\n",
        "  #print(accuracyscore * 100)\n",
        "  #print(\"Test loss:\", accuracyscore[0])\n",
        "  #print(\"Test accuracy:\", accuracyscore[1] * 100)\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvA8T3Bz1brG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6TAWj3d3a7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5b172e-33bc-469a-ef33-add27a93fa6d"
      },
      "source": [
        "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "\n",
        "#y_binary = to_categorical(y_int)\n",
        "def Bidirectionalfunction(padded_train_sequences, Y_train):\n",
        "    embedding_dim = 150\n",
        "    embedding_matrix = np.random.random((MAX_NB_WORDS, embedding_dim))\n",
        "    \n",
        "    inp = Input(shape=(MAX_LENGTH, ))\n",
        "    x = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH, \n",
        "                  weights=[embedding_matrix], trainable=True)(inp)\n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "    x = Bidirectional(GRU(10, return_sequences=True))(x)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    outp = Dense(2, activation=\"sigmoid\")(conc)\n",
        "    \n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    history = model.fit(x=padded_train_sequences, y=Y_train, \n",
        "                    #validation_data=(padded_test_sequences, Y_test), \n",
        "                    batch_size=batch_size, \n",
        "                    #callbacks=[checkpoint], \n",
        "                    epochs=30, \n",
        "                    verbose=1)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = Bidirectionalfunction(padded_train_sequences, Y_train)\n",
        "\n",
        "accuracyscore = model.evaluate( padded_test_sequences, Y_test)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "print(accuracyscore * 100)\n",
        "print(\"Test loss:\", accuracyscore[0])\n",
        "print(\"Test accuracy:\", accuracyscore[1] * 100)\n",
        "\n",
        "y_pred = model.predict([padded_test_sequences])\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, np.argmax(y_pred, axis = 1)))\n",
        "print(classification_report(y_test, np.argmax(y_pred, axis = 1)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1308/1308 [==============================] - 237s 179ms/step - loss: 0.5312 - accuracy: 0.7083\n",
            "Epoch 2/30\n",
            "1308/1308 [==============================] - 245s 188ms/step - loss: 0.3088 - accuracy: 0.8625\n",
            "Epoch 3/30\n",
            "1308/1308 [==============================] - 236s 181ms/step - loss: 0.2307 - accuracy: 0.9070\n",
            "Epoch 4/30\n",
            "1308/1308 [==============================] - 231s 176ms/step - loss: 0.1659 - accuracy: 0.9380\n",
            "Epoch 5/30\n",
            "1308/1308 [==============================] - 235s 180ms/step - loss: 0.1208 - accuracy: 0.9568\n",
            "Epoch 6/30\n",
            "1308/1308 [==============================] - 241s 184ms/step - loss: 0.0908 - accuracy: 0.9686\n",
            "Epoch 7/30\n",
            "1308/1308 [==============================] - 241s 184ms/step - loss: 0.0736 - accuracy: 0.9763\n",
            "Epoch 8/30\n",
            "1308/1308 [==============================] - 243s 186ms/step - loss: 0.0586 - accuracy: 0.9811\n",
            "Epoch 9/30\n",
            "1308/1308 [==============================] - 242s 185ms/step - loss: 0.0493 - accuracy: 0.9848\n",
            "Epoch 10/30\n",
            "1308/1308 [==============================] - 233s 178ms/step - loss: 0.0432 - accuracy: 0.9858\n",
            "Epoch 11/30\n",
            "1308/1308 [==============================] - 235s 179ms/step - loss: 0.0335 - accuracy: 0.9894\n",
            "Epoch 12/30\n",
            "1308/1308 [==============================] - 232s 177ms/step - loss: 0.0330 - accuracy: 0.9893\n",
            "Epoch 13/30\n",
            "1308/1308 [==============================] - 234s 179ms/step - loss: 0.0272 - accuracy: 0.9921\n",
            "Epoch 14/30\n",
            "1308/1308 [==============================] - 237s 181ms/step - loss: 0.0227 - accuracy: 0.9928\n",
            "Epoch 15/30\n",
            "1308/1308 [==============================] - 239s 183ms/step - loss: 0.0230 - accuracy: 0.9927\n",
            "Epoch 16/30\n",
            "1308/1308 [==============================] - 228s 174ms/step - loss: 0.0188 - accuracy: 0.9938\n",
            "Epoch 17/30\n",
            "1308/1308 [==============================] - 227s 174ms/step - loss: 0.0174 - accuracy: 0.9938\n",
            "Epoch 18/30\n",
            "1308/1308 [==============================] - 224s 171ms/step - loss: 0.0180 - accuracy: 0.9939\n",
            "Epoch 19/30\n",
            "1308/1308 [==============================] - 231s 176ms/step - loss: 0.0167 - accuracy: 0.9942\n",
            "Epoch 20/30\n",
            "1308/1308 [==============================] - 227s 174ms/step - loss: 0.0145 - accuracy: 0.9950\n",
            "Epoch 21/30\n",
            "1308/1308 [==============================] - 233s 178ms/step - loss: 0.0135 - accuracy: 0.9952\n",
            "Epoch 22/30\n",
            "1308/1308 [==============================] - 231s 176ms/step - loss: 0.0139 - accuracy: 0.9950\n",
            "Epoch 23/30\n",
            "1308/1308 [==============================] - 235s 179ms/step - loss: 0.0135 - accuracy: 0.9955\n",
            "Epoch 24/30\n",
            "1308/1308 [==============================] - 244s 186ms/step - loss: 0.0139 - accuracy: 0.9952\n",
            "Epoch 25/30\n",
            "1308/1308 [==============================] - 240s 184ms/step - loss: 0.0124 - accuracy: 0.9954\n",
            "Epoch 26/30\n",
            "1308/1308 [==============================] - 236s 180ms/step - loss: 0.0126 - accuracy: 0.9957\n",
            "Epoch 27/30\n",
            "1308/1308 [==============================] - 231s 177ms/step - loss: 0.0117 - accuracy: 0.9957\n",
            "Epoch 28/30\n",
            "1308/1308 [==============================] - 231s 177ms/step - loss: 0.0112 - accuracy: 0.9958\n",
            "Epoch 29/30\n",
            "1308/1308 [==============================] - 233s 178ms/step - loss: 0.0114 - accuracy: 0.9963\n",
            "Epoch 30/30\n",
            "1308/1308 [==============================] - 230s 176ms/step - loss: 0.0115 - accuracy: 0.9955\n",
            "436/436 [==============================] - 11s 24ms/step - loss: 0.6359 - accuracy: 0.8744\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 343)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 343, 150)     12000000    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d (SpatialDropo (None, 343, 150)     0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 343, 20)      9720        spatial_dropout1d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 20)           0           bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 20)           0           bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 40)           0           global_average_pooling1d[0][0]   \n",
            "                                                                 global_max_pooling1d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            82          concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 12,009,802\n",
            "Trainable params: 12,009,802\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553, 0.6358780264854431, 0.8743816018104553]\n",
            "Test loss: 0.6358780264854431\n",
            "Test accuracy: 87.43816018104553\n",
            "[[4947 1153]\n",
            " [ 599 7248]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.81      0.85      6100\n",
            "           1       0.86      0.92      0.89      7847\n",
            "\n",
            "    accuracy                           0.87     13947\n",
            "   macro avg       0.88      0.87      0.87     13947\n",
            "weighted avg       0.88      0.87      0.87     13947\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRF7RyLfAbBz"
      },
      "source": [
        "# define the model\n",
        "def define_model3(padded_train_sequences, Y_train):\n",
        "\t# channel 1\n",
        "\tembedding_dim = 150\n",
        "\tinputs1 = Input(shape=(MAX_LENGTH,))\n",
        "\tembedding1 = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH)(inputs1)\n",
        "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "\tdrop1 = Dropout(0.5)(conv1)\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "\tflat1 = Flatten()(pool1)\n",
        "\t# channel 2\n",
        "\tinputs2 = Input(shape=(MAX_LENGTH,))\n",
        "\tembedding2 = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH)(inputs2)\n",
        "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "\tdrop2 = Dropout(0.5)(conv2)\n",
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "\tflat2 = Flatten()(pool2)\n",
        "\t# channel 3\n",
        "\tinputs3 = Input(shape=(MAX_LENGTH,))\n",
        "\tembedding3 = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH)(inputs3)\n",
        "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "\tdrop3 = Dropout(0.5)(conv3)\n",
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "\tflat3 = Flatten()(pool3)\n",
        "\t# merge\n",
        "\tmerged = concatenate([flat1, flat2, flat3])\n",
        "\t# interpretation\n",
        "\tdense1 = Dense(10, activation='relu')(merged)\n",
        "\toutputs = Dense(2, activation='sigmoid')(dense1)\n",
        "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\t# compile\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\tmodel.fit([padded_train_sequences,padded_train_sequences,padded_train_sequences], y_train, epochs=3, batch_size=16)\n",
        "\n",
        "\t#print(model.summary())\n",
        "\treturn model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riLwwCieNCXf"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "Y = data.label\n",
        "lebEnc = LabelEncoder()\n",
        "Y = lebEnc.fit_transform(Y)\n",
        "Y = Y.reshape(-1,1)\n",
        "\n",
        "\n",
        "X = data.full_text\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.25)\n",
        "\n",
        "MAX_NB_WORDS = 80000\n",
        "MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in X_train]) \n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "\n",
        "\n",
        "MAX_LENGTH = max([len(s.split()) for s in X_train])\n",
        "padded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\n",
        "padded_test_sequences = pad_sequences(test_sequences, maxlen=MAX_LENGTH)\n",
        "\n",
        "\n",
        "Y_test = keras.utils.to_categorical(np.asarray(y_test))\n",
        "Y_train = keras.utils.to_categorical(np.asarray(y_train))\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm_s7RGsLQ6Y",
        "outputId": "b27aa9e8-5ead-4242-f112-ee20f66ed441"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13947, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "rNXnPDUNJcrR",
        "outputId": "5e55dc47-7830-4730-fb2d-42945d538471"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 32 \n",
        "Y = data3.annotation\n",
        "lebEnc = LabelEncoder()\n",
        "Y = lebEnc.fit_transform(Y)\n",
        "Y = Y.reshape(-1,1)\n",
        "\n",
        "\n",
        "X = data3.content\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.90)\n",
        "\n",
        "MAX_NB_WORDS = 80000\n",
        "MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in X_train]) \n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "\n",
        "\n",
        "MAX_LENGTH = max([len(s.split()) for s in X_train])\n",
        "padded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\n",
        "padded_test_sequences = pad_sequences(test_sequences, maxlen=MAX_LENGTH)\n",
        "\n",
        "\n",
        "Y_test = keras.utils.to_categorical(np.asarray(y_test))\n",
        "Y_train = keras.utils.to_categorical(np.asarray(y_train))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-f50f2fbcf7a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mlebEnc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlebEnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'annotation'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "1ldPbps39v7q",
        "outputId": "9f149b50-bc8a-4383-d968-604c6d8cdc38"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import keras\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "Y = data.label\r\n",
        "lebEnc = LabelEncoder()\r\n",
        "Y = lebEnc.fit_transform(Y)\r\n",
        "Y = Y.reshape(-1,1)\r\n",
        "\r\n",
        "\r\n",
        "X = data.full_text\r\n",
        "\r\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.25)\r\n",
        "\r\n",
        "MAX_NB_WORDS = 80000\r\n",
        "MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in X_train]) \r\n",
        "\r\n",
        "\r\n",
        "# Instantiate the CountVectorizer method\r\n",
        "count_vector = CountVectorizer(stop_words = 'english', lowercase = True)\r\n",
        "\r\n",
        "# Fit the training data and then return the matrix\r\n",
        "padded_train_sequences = count_vector.fit_transform(X_train)\r\n",
        "\r\n",
        "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\r\n",
        "padded_test_sequences = count_vector.transform(X_test)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\r\n",
        "#tokenizer.fit_on_texts(X_train)\r\n",
        "\r\n",
        "\r\n",
        "#train_sequences = tokenizer.texts_to_sequences(X_train)\r\n",
        "#test_sequences = tokenizer.texts_to_sequences(X_test)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#MAX_LENGTH = max([len(s.split()) for s in X_train])\r\n",
        "#padded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\r\n",
        "#padded_test_sequences = pad_sequences(test_sequences, maxlen=MAX_LENGTH)\r\n",
        "\r\n",
        "\r\n",
        "Y_test = keras.utils.to_categorical(np.asarray(y_test))\r\n",
        "Y_train = keras.utils.to_categorical(np.asarray(y_train))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a956cd6e50c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mMAX_NB_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-a956cd6e50c2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mMAX_NB_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pheZYd-lgpz-",
        "outputId": "6f29b51f-7200-44fb-d315-8b9111eef3e7"
      },
      "source": [
        "print(Y_train.shape) \n",
        "print(Y_test.shape)\n",
        "print(Y_train.shape[0] + Y_test.shape[0])\n",
        "print(MAX_LENGTH)\n",
        "\n",
        "\n",
        "print('-------------------------')\n",
        "\n",
        "print(55788 * 0.25)\n",
        "print(55788 * 0.75)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(41841, 2)\n",
            "(13947, 2)\n",
            "55788\n",
            "343\n",
            "-------------------------\n",
            "13947.0\n",
            "41841.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "tKWQptmkXwVq",
        "outputId": "62da96e5-1748-4b93-d71a-7d717ffba8ad"
      },
      "source": [
        "modelv = tf.keras.models.load_model('content/saved_model/model_1')\n",
        "\n",
        "accuracyscore = modelv.evaluate(padded_test_sequences, Y_test)\n",
        "\n",
        "print(accuracyscore * 100)\n",
        "print(\"Test loss:\", accuracyscore[0])\n",
        "print(\"Test accuracy:\", accuracyscore[1] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "224/224 [==============================] - 2s 10ms/step - loss: 0.6124 - accuracy: 0.9063\n",
            "[0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372, 0.6124063730239868, 0.9062587022781372]\n",
            "Test loss: 0.6124063730239868\n",
            "Test accuracy: 90.62587022781372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p60vXzyIdRf5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9tkkdSEs2vzH",
        "outputId": "61a93bf3-4f91-42cd-fd95-316748072e88"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "#model_save_name = 'classifier.pt'\n",
        "#path = F\"/content/gdrive/My Drive/{filename}\" \n",
        "\n",
        "!mkdir -p saved_model\n",
        "n_members = 1\n",
        "for i in range(n_members):\n",
        "\t# fit model\n",
        "\tmodel = fit_model(padded_train_sequences, Y_train)\n",
        "\t# save model\n",
        "\tpath = 'content/saved_model/model_' + str(i + 1) + ''\n",
        "\tmodel.save(path)\n",
        "\tprint('>Saved %s' % path)\n",
        " \n",
        " \n",
        "\n",
        "\n",
        "model = define_model3(padded_train_sequences, y_train)\n",
        "\n",
        "model.save('content/saved_model/model_2')\n",
        "print('>Saved as model_2')\n",
        "\n",
        "\n",
        "model = Bidirectionalfunction(padded_train_sequences, Y_train)\n",
        "\n",
        "model.save('content/saved_model/model_3')\n",
        "print('>Saved as model_3')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "1432/1432 [==============================] - 44s 31ms/step - loss: 0.2954 - accuracy: 0.8650\n",
            "Epoch 2/8\n",
            "1432/1432 [==============================] - 46s 32ms/step - loss: 0.1376 - accuracy: 0.9488\n",
            "Epoch 3/8\n",
            "1432/1432 [==============================] - 45s 32ms/step - loss: 0.0739 - accuracy: 0.9738\n",
            "Epoch 4/8\n",
            "1432/1432 [==============================] - 45s 32ms/step - loss: 0.0443 - accuracy: 0.9846\n",
            "Epoch 5/8\n",
            "1432/1432 [==============================] - 45s 32ms/step - loss: 0.0284 - accuracy: 0.9906\n",
            "Epoch 6/8\n",
            "1432/1432 [==============================] - 46s 32ms/step - loss: 0.0209 - accuracy: 0.9931\n",
            "Epoch 7/8\n",
            "1432/1432 [==============================] - 45s 32ms/step - loss: 0.0164 - accuracy: 0.9941\n",
            "Epoch 8/8\n",
            "1432/1432 [==============================] - 46s 32ms/step - loss: 0.0124 - accuracy: 0.9952\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: content/saved_model/model_1/assets\n",
            ">Saved content/saved_model/model_1\n",
            "Epoch 1/10\n",
            "1790/1790 [==============================] - 227s 127ms/step - loss: 0.3683 - accuracy: 0.8389\n",
            "Epoch 2/10\n",
            "1790/1790 [==============================] - 227s 127ms/step - loss: 0.2130 - accuracy: 0.9194\n",
            "Epoch 3/10\n",
            "1790/1790 [==============================] - 228s 127ms/step - loss: 0.1119 - accuracy: 0.9612\n",
            "Epoch 4/10\n",
            "1790/1790 [==============================] - 228s 127ms/step - loss: 0.0484 - accuracy: 0.9833\n",
            "Epoch 5/10\n",
            "1790/1790 [==============================] - 228s 127ms/step - loss: 0.0246 - accuracy: 0.9919\n",
            "Epoch 6/10\n",
            "1790/1790 [==============================] - 228s 127ms/step - loss: 0.0159 - accuracy: 0.9946\n",
            "Epoch 7/10\n",
            "1790/1790 [==============================] - 228s 128ms/step - loss: 0.0121 - accuracy: 0.9955\n",
            "Epoch 8/10\n",
            "1790/1790 [==============================] - 228s 128ms/step - loss: 0.0122 - accuracy: 0.9958\n",
            "Epoch 9/10\n",
            "1790/1790 [==============================] - 228s 127ms/step - loss: 0.0100 - accuracy: 0.9963\n",
            "Epoch 10/10\n",
            "1790/1790 [==============================] - 228s 127ms/step - loss: 0.0083 - accuracy: 0.9965\n",
            "INFO:tensorflow:Assets written to: content/saved_model/model_2/assets\n",
            ">Saved as model_2\n",
            "Epoch 1/8\n",
            "1432/1432 [==============================] - 268s 187ms/step - loss: 0.3263 - accuracy: 0.8551\n",
            "Epoch 2/8\n",
            "1432/1432 [==============================] - 274s 192ms/step - loss: 0.1838 - accuracy: 0.9277\n",
            "Epoch 3/8\n",
            "1432/1432 [==============================] - 273s 191ms/step - loss: 0.1323 - accuracy: 0.9477\n",
            "Epoch 4/8\n",
            "1432/1432 [==============================] - 268s 187ms/step - loss: 0.0849 - accuracy: 0.9697\n",
            "Epoch 5/8\n",
            "1432/1432 [==============================] - 273s 191ms/step - loss: 0.0571 - accuracy: 0.9802\n",
            "Epoch 6/8\n",
            "1432/1432 [==============================] - 265s 185ms/step - loss: 0.0390 - accuracy: 0.9867\n",
            "Epoch 7/8\n",
            "1432/1432 [==============================] - 261s 182ms/step - loss: 0.0292 - accuracy: 0.9904\n",
            "Epoch 8/8\n",
            "1432/1432 [==============================] - 264s 184ms/step - loss: 0.0214 - accuracy: 0.9929\n",
            "INFO:tensorflow:Assets written to: content/saved_model/model_3/assets\n",
            ">Saved as model_3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eoAvYMXqI8P"
      },
      "source": [
        "model = tf.keras.models.load_model(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VOPV7IJtcqq3",
        "outputId": "27fda6dc-0462-47e1-fbea-7aa85caa1d44"
      },
      "source": [
        "# stacked generalization with linear meta model on blobs dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from numpy import dstack\n",
        "\n",
        "# load models from file\n",
        "def load_all_models(n_models):\n",
        "\tall_models = list()\n",
        "\tfor i in range(n_models):\n",
        "\t\t# define filename for this ensemble\n",
        "\t\tfilename = 'content/saved_model/model_' + str(i + 1) + ''\n",
        "\t\t# load model from file\n",
        "\t\tmodel = tf.keras.models.load_model(filename)\n",
        "\t\t# add to list of members\n",
        "\t\tall_models.append(model)\n",
        "\t\tprint('>loaded %s' % filename)\n",
        "\treturn all_models\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX):\n",
        "\tstackX = None\n",
        "\tfor model in members:\n",
        "\t\t# make prediction\n",
        "\t\tyhat = model.predict(inputX, verbose=0)\n",
        "\t\t# stack predictions into [rows, members, probabilities]\n",
        "\t\tif stackX is None:\n",
        "\t\t\tstackX = yhat\n",
        "\t\telse:\n",
        "\t\t\tstackX = dstack((stackX, yhat))\n",
        "\t# flatten predictions to [rows, members x probabilities]\n",
        "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "\treturn stackX\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model(members, inputX, inputy):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\t# fit standalone model\n",
        "\tmodel = SGDClassifier()\n",
        "\tmodel.fit(stackedX, inputy)\n",
        "\treturn model\n",
        "\n",
        "# make a prediction with the stacked model\n",
        "def stacked_prediction(members, model, inputX):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\t# make a prediction\n",
        "\tyhat = model.predict(stackedX)\n",
        "\treturn yhat\n",
        "\n",
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "# split into train and test\n",
        "#n_train = 100\n",
        "#trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "#trainy, testy = y[:n_train], y[n_train:]\n",
        "print(padded_train_sequences.shape, padded_test_sequences.shape)\n",
        "# load all models\n",
        "n_members = 3\n",
        "members = load_all_models(n_members)\n",
        "print('Loaded %d models' % len(members))\n",
        "# evaluate standalone models on test dataset\n",
        "for model in members:\n",
        "\t#testy_enc = to_categorical(testy)\n",
        "\tprint(model.name)\n",
        "\tif model.name == \"functional_2\":\n",
        "\t\t_, acc = model.evaluate(padded_test_sequences, y_test, verbose=1)\n",
        "\tif model.name == \"functional_1\":\n",
        "\t\t_, acc = model.evaluate(padded_test_sequences, Y_test, verbose=1)\n",
        "\tif model.name == \"functional_5\":\n",
        "\t\t_, acc = model.evaluate(padded_test_sequences, Y_test, verbose=1)\n",
        "\tprint('Model Accuracy: %.3f' % acc)\n",
        "# fit stacked model using the ensemble\n",
        "model = fit_stacked_model(members, padded_test_sequences, y_test)\n",
        "# evaluate model on test set\n",
        "yhat = stacked_prediction(members, model, padded_test_sequences)\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Stacked Test Accuracy: %.3f' % acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28629, 150) (7158, 150)\n",
            ">loaded content/saved_model/model_1\n",
            ">loaded content/saved_model/model_2\n",
            ">loaded content/saved_model/model_3\n",
            "Loaded 3 models\n",
            "functional_1\n",
            "224/224 [==============================] - 3s 12ms/step - loss: 0.6167 - accuracy: 0.9012\n",
            "Model Accuracy: 0.901\n",
            "functional_3\n",
            "Model Accuracy: 0.901\n",
            "functional_5\n",
            "224/224 [==============================] - 4s 16ms/step - loss: 0.2894 - accuracy: 0.9142\n",
            "Model Accuracy: 0.914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-1be32752ae5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Accuracy: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# fit stacked model using the ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_stacked_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_test_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;31m# evaluate model on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_test_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-1be32752ae5b>\u001b[0m in \u001b[0;36mfit_stacked_model\u001b[0;34m(members, inputX, inputy)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_stacked_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# create dataset using ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mstackedX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# fit standalone model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-1be32752ae5b>\u001b[0m in \u001b[0;36mstacked_dataset\u001b[0;34m(members, inputX)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;31m# make prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# stack predictions into [rows, members, probabilities]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstackX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1418 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:517 _run_internal_graph\n        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n\n    AssertionError: Could not compute output Tensor(\"dense_9/Sigmoid_13:0\", shape=(None, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nKDWS9jZ4oP5",
        "outputId": "b7a55cc8-1ca4-4e71-8b79-7f0be8b08ffa"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x7ff6a62a8828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZV1afjsHLX-"
      },
      "source": [
        "# define the model\n",
        "def define_Ch3model(padded_train_sequences, y_train):\n",
        "\t# channel 1\n",
        "  embed_dim = 40  # Embedding size for each token\n",
        "  num_heads = 4  # Number of attention heads\n",
        "  ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "  maxlen = MAX_SEQUENCE_LENGTH\n",
        "  vocab_size = vocab_size = len(tokenizer.word_index) + 1 #80000\n",
        "  print(vocab_size)\n",
        "  inputs1 = layers.Input(shape=(maxlen,))\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "  x1 = embedding_layer(inputs1)\n",
        "  transformer_block = TransformerBlock(embed_dim, 2, ff_dim)\n",
        "  x1 = transformer_block(x1)\n",
        "  x1= layers.MaxPooling1D()(x1)\n",
        " # x1 = layers.Dropout(0.1)(x1)\n",
        "  x1 = layers.Dense(30, activation=\"relu\")(x1)\n",
        "  x1 = layers.Dropout(0.1)(x1)\n",
        "  x1 = Flatten()(x1)\n",
        "\t# channel 2\n",
        "  inputs2 = Input(shape=(MAX_LENGTH,))\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "  x2 = embedding_layer(inputs2)\n",
        "  transformer_block = TransformerBlock(embed_dim, 10, ff_dim)\n",
        "  x2 = transformer_block(x2)\n",
        "  x2= layers.MaxPooling1D()(x2)\n",
        "  #x2 = layers.Conv1D(filters=32, kernel_size=4, activation='relu')(x2)\n",
        "  #x2 = layers.Dropout(0.1)(x2)\n",
        "  x2 = layers.Dense(30, activation=\"relu\")(x2)\n",
        "  x2 = layers.Dropout(0.1)(x2)\n",
        "  x2 = Flatten()(x2)\n",
        "\t# channel 3\n",
        "  inputs3 = Input(shape=(MAX_LENGTH,))\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "  x3 = embedding_layer(inputs3)\n",
        "  transformer_block = TransformerBlock(embed_dim, 8, ff_dim)\n",
        "  x3 = transformer_block(x3)\n",
        "  #x3 = transformer_block(x3)\n",
        "  #x3 = transformer_block(x3)\n",
        "  x3= layers.MaxPooling1D()(x3)\n",
        "  #x3 = layers.Dropout(0.1)(x3)\n",
        "  x3 = layers.Dense(30, activation=\"relu\")(x3)\n",
        "  x3 = layers.Dropout(0.1)(x3)\n",
        "  x3 = Flatten()(x3)\n",
        "\t# merge\n",
        "  merged = concatenate([x1, x2, x3])\n",
        "  #merged1 = Flatten()(merged1)\n",
        "\t# interpretation\n",
        "\n",
        "  #merged = Conv1D(filters=(3), kernel_size=(20), activation='relu')(merged1)\n",
        "  dense1 = Dense(60, activation='relu')(merged)\n",
        "  dense2 = Dense(30, activation='relu')(dense1)\n",
        "  outputs = Dense(2, activation='softmax')(dense2)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  \n",
        "\t# compile\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  #model.fit([padded_train_sequences,padded_train_sequences,padded_train_sequences], y_train, epochs=3, batch_size=16)\n",
        "  history = model.fit(x=[padded_train_sequences,padded_train_sequences, padded_train_sequences], y=Y_train, \n",
        "                    #validation_data=(padded_test_sequences, Y_test), \n",
        "                    batch_size=200, \n",
        "                    #callbacks=[checkpoint], \n",
        "                    epochs=40, \n",
        "                    verbose=1)\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3orfRKjY5r9O"
      },
      "source": [
        "RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "id": "djRUxJVhJYuo",
        "outputId": "023fc229-a944-4650-cf2c-a186cb2b747e"
      },
      "source": [
        "#model = define_Ch3model(padded_train_sequences, Y_train)\r\n",
        "model = define_model3(padded_train_sequences, Y_train)\r\n",
        "#define_model3\r\n",
        "\r\n",
        "accuracyscore = model.evaluate([padded_test_sequences,padded_test_sequences, padded_test_sequences], Y_test)\r\n",
        "#accuracyscore = model.evaluate( padded_test_sequences, Y_test)\r\n",
        "\r\n",
        "print(accuracyscore * 100)\r\n",
        "print(\"Test loss:\", accuracyscore[0])\r\n",
        "print(\"Test accuracy:\", accuracyscore[1] * 100)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1544c4d96e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model = define_Ch3model(padded_train_sequences, Y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_train_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#define_model3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracyscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpadded_test_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadded_test_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_test_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-cd292e1cf000>\u001b[0m in \u001b[0;36mdefine_model3\u001b[0;34m(padded_train_sequences, Y_train)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpadded_train_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadded_train_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadded_train_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#print(model.summary())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1608 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4979 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:174 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 2) vs (None, 1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GWLbC9UadE4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBqLm1kGONsG",
        "outputId": "fcbd71ee-4766-4bcb-bfab-100117169fb0"
      },
      "source": [
        "\n",
        "accuracyscore = model.evaluate([padded_test_sequences,padded_test_sequences, padded_test_sequences], Y_test)\n",
        "#accuracyscore = model.evaluate( padded_test_sequences, Y_test)\n",
        "\n",
        "print(accuracyscore * 100)\n",
        "print(\"Test loss:\", accuracyscore[0])\n",
        "print(\"Test accuracy:\", accuracyscore[1] * 100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "436/436 [==============================] - 23s 53ms/step - loss: 0.9081 - accuracy: 0.8758\n",
            "[0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617, 0.9080589413642883, 0.8758155703544617]\n",
            "Test loss: 0.9080589413642883\n",
            "Test accuracy: 87.58155703544617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "tMN2FlmgWV-4",
        "outputId": "f103963d-74ce-44dc-8fd8-a272818c69aa"
      },
      "source": [
        "y_pred = model.predict([padded_test_sequences,padded_test_sequences,padded_test_sequences])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b2819f224f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpadded_test_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadded_test_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadded_test_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:207 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer model expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 210) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 210) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 210) dtype=int32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "5JjaWecNWbMR",
        "outputId": "e8d3b831-2809-42c9-80e0-7dbd813b0054"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "matrix = confusion_matrix(Y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "\n",
        "print(matrix)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2187  299]\n",
            " [ 337 4335]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "5350YAzTXNw_",
        "outputId": "89c8723d-ce57-4e7a-d973-5de645bf4da4"
      },
      "source": [
        "train_loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "train_acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-9971176eb3d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j-UHNgxDJTOb",
        "outputId": "4152ed4a-cd8f-4bc7-8dbc-515053d862ec"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_21 (InputLayer)           [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_22 (InputLayer)           [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_23 (InputLayer)           [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_and_position_embedding_16 (None, 150, 32)      2564800     input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "token_and_position_embedding_17 (None, 150, 32)      2564800     input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "token_and_position_embedding_18 (None, 150, 32)      2564800     input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_16 (Transform (None, 150, 32)      6464        token_and_position_embedding_16[0\n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_17 (Transform (None, 150, 32)      6464        token_and_position_embedding_17[0\n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_18 (Transform (None, 150, 32)      6464        token_and_position_embedding_18[0\n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_17 (Gl (None, 32)           0           transformer_block_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_18 (Gl (None, 32)           0           transformer_block_17[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_19 (Gl (None, 32)           0           transformer_block_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_69 (Dropout)            (None, 32)           0           global_average_pooling1d_17[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_73 (Dropout)            (None, 32)           0           global_average_pooling1d_18[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_77 (Dropout)            (None, 32)           0           global_average_pooling1d_19[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_132 (Dense)               (None, 200)          6600        dropout_69[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_139 (Dense)               (None, 200)          6600        dropout_73[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_146 (Dense)               (None, 200)          6600        dropout_77[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_70 (Dropout)            (None, 200)          0           dense_132[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_74 (Dropout)            (None, 200)          0           dense_139[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_78 (Dropout)            (None, 200)          0           dense_146[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 600)          0           dropout_70[0][0]                 \n",
            "                                                                 dropout_74[0][0]                 \n",
            "                                                                 dropout_78[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_147 (Dense)               (None, 20)           12020       concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_148 (Dense)               (None, 2)            42          dense_147[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 7,745,654\n",
            "Trainable params: 7,745,654\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tJZ-XQ4MJmoA",
        "outputId": "609630a2-8af8-4bb7-d32f-b471789ce4a2"
      },
      "source": [
        "\t# channel 1\n",
        "embed_dim = 16  # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "maxlen = MAX_SEQUENCE_LENGTH\n",
        "vocab_size = vocab_size = len(tokenizer.word_index) + 1 #80000\n",
        "inputs1 = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x1 = embedding_layer(inputs1)\n",
        "transformer_block = TransformerBlock(embed_dim, 2, ff_dim)\n",
        "x1 = transformer_block(x1)\n",
        "x1= layers.GlobalAveragePooling1D()(x1)\n",
        "x1 = layers.Dropout(0.1)(x1)\n",
        "x1 = layers.Dense(20, activation=\"relu\")(x1)\n",
        "outputs = layers.Dropout(0.1)(x1)\n",
        "model = Model(inputs=inputs1, outputs=outputs)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  #model.fit([padded_train_sequences,padded_train_sequences,padded_train_sequences], y_train, epochs=3, batch_size=16)\n",
        "history = model.fit(x=padded_train_sequences, y=Y_train, \n",
        "                    #validation_data=(padded_test_sequences, Y_test), \n",
        "                    batch_size=batch_size, \n",
        "                    #callbacks=[checkpoint], \n",
        "                    epochs=30, \n",
        "                    verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-e0af3f494645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0;31m#callbacks=[checkpoint],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1605 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4829 binary_crossentropy\n        bce = target * math_ops.log(output + epsilon())\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1141 binary_op_wrapper\n        raise e\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1125 binary_op_wrapper\n        return func(x, y, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1457 _mul_dispatch\n        return multiply(x, y, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:509 multiply\n        return gen_math_ops.mul(x, y, name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:6176 mul\n        \"Mul\", x=x, y=y, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n        compute_device)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1975 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 2 and 20 for '{{node binary_crossentropy/mul}} = Mul[T=DT_FLOAT](IteratorGetNext:1, binary_crossentropy/Log)' with input shapes: [?,2], [?,20].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "jKlBpZvs3dpF",
        "outputId": "c393815e-c3f7-481e-ff4b-e4ae0f1d6355"
      },
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "maxlen = MAX_SEQUENCE_LENGTH\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  #model.fit([padded_train_sequences,padded_train_sequences,padded_train_sequences], y_train, epochs=3, batch_size=16)\n",
        "history = model.fit(x=padded_train_sequences, y=Y_train, \n",
        "                    #validation_data=(padded_test_sequences, Y_test), \n",
        "                    batch_size=batch_size, \n",
        "                    #callbacks=[checkpoint], \n",
        "                    epochs=30, \n",
        "                    verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-d2395c230c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0;31m#callbacks=[checkpoint],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[21,335] = 11239 is not in [0, 8438)\n\t [[node functional_29/token_and_position_embedding_19/embedding_38/embedding_lookup (defined at <ipython-input-18-eb4e2f62143a>:95) ]] [Op:__inference_train_function_70245]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_29/token_and_position_embedding_19/embedding_38/embedding_lookup:\n functional_29/token_and_position_embedding_19/embedding_38/embedding_lookup/69239 (defined at /usr/lib/python3.6/contextlib.py:81)\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZYQ_nFOZIu-",
        "outputId": "b3a904e8-56c0-4244-f8d5-8f8c758ef180"
      },
      "source": [
        "accuracyscore = model.evaluate(padded_test_sequences, Y_test)\n",
        "\n",
        "print(accuracyscore * 100)\n",
        "print(\"Test loss:\", accuracyscore[0])\n",
        "print(\"Test accuracy:\", accuracyscore[1] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 6s 12ms/step - loss: 1.8011 - accuracy: 0.7212\n",
            "[1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176, 1.8010724782943726, 0.7212185859680176]\n",
            "Test loss: 1.8010724782943726\n",
            "Test accuracy: 72.12185859680176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRazpDElXVGX"
      },
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        " \n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        " \n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        " \n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        " \n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "# load all training reviews\n",
        "positive_lines = process_docs('txt_sentoken/pos', vocab, True)\n",
        "# = process_docs('txt_sentoken/neg', vocab, True)\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "docs = negative_lines + positive_lines\n",
        "tokenizer.fit_on_texts(docs)\n",
        "# encode training data set\n",
        "Xtrain = tokenizer.texts_to_matrix(docs, mode='freq')\n",
        "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
        " \n",
        "# load all test reviews\n",
        "positive_lines = process_docs('txt_sentoken/pos', vocab, False)\n",
        "negative_lines = process_docs('txt_sentoken/neg', vocab, False)\n",
        "docs = negative_lines + positive_lines\n",
        "# encode training data set\n",
        "Xtest = tokenizer.texts_to_matrix(docs, mode='freq')\n",
        "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
        " \n",
        "n_words = Xtest.shape[1]\n",
        "# define network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=50, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEnmdOvlFex2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afe9e7b-81d0-4e41-86a4-f5a8b3857cfd"
      },
      "source": [
        "# define the model\r\n",
        "def define_Ch3model(padded_train_sequences, y_train):\r\n",
        "\t# channel 1\r\n",
        "  embed_dim = 40  # Embedding size for each token\r\n",
        "  num_heads = 4  # Number of attention heads\r\n",
        "  ff_dim = 32  # Hidden layer size in feed forward network inside transformer\r\n",
        "  maxlen = MAX_SEQUENCE_LENGTH\r\n",
        "  vocab_size = vocab_size = len(tokenizer.word_index) + 1 #80000\r\n",
        "  print(vocab_size)\r\n",
        "  inputs1 = layers.Input(shape=(maxlen,))\r\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\n",
        "  x1 = embedding_layer(inputs1)\r\n",
        "  transformer_block = TransformerBlock(embed_dim, 2, ff_dim)\r\n",
        "  x1 = transformer_block(x1)\r\n",
        "  x1= layers.MaxPooling1D()(x1)\r\n",
        " # x1 = layers.Dropout(0.1)(x1)\r\n",
        "  x1 = layers.Dense(30, activation=\"relu\")(x1)\r\n",
        "  x1 = layers.Dropout(0.1)(x1)\r\n",
        "  x1 = Flatten()(x1)\r\n",
        "\t# channel 2\r\n",
        "\t# merge\r\n",
        " \r\n",
        "  #merged1 = Flatten()(merged1)\r\n",
        "\t# interpretation\r\n",
        "\r\n",
        "  #merged = Conv1D(filters=(3), kernel_size=(20), activation='relu')(merged1)\r\n",
        "  dense1 = Dense(40, activation='relu')(x1)\r\n",
        "  dense2 = Dense(20, activation='relu')(dense1)\r\n",
        "  outputs = Dense(2, activation='softmax')(dense2)\r\n",
        "  model = Model(inputs=inputs1, outputs=outputs)\r\n",
        "  \r\n",
        "\t# compile\r\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "  #model.fit([padded_train_sequences,padded_train_sequences,padded_train_sequences], y_train, epochs=3, batch_size=16)\r\n",
        "  history = model.fit(x=[padded_train_sequences ], y=Y_train, \r\n",
        "                    #validation_data=(padded_test_sequences, Y_test), \r\n",
        "                    batch_size=200, \r\n",
        "                    #callbacks=[checkpoint], \r\n",
        "                    epochs=15, \r\n",
        "                    verbose=1)\r\n",
        "  print(model.summary())\r\n",
        "  return model\r\n",
        "\r\n",
        "model = define_model3(padded_train_sequences, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "210/210 [==============================] - 20s 91ms/step - loss: 0.5774 - accuracy: 0.6569\n",
            "Epoch 2/40\n",
            "210/210 [==============================] - 19s 88ms/step - loss: 0.2360 - accuracy: 0.9082\n",
            "Epoch 3/40\n",
            "210/210 [==============================] - 18s 86ms/step - loss: 0.1335 - accuracy: 0.9523\n",
            "Epoch 4/40\n",
            "210/210 [==============================] - 18s 87ms/step - loss: 0.0804 - accuracy: 0.9734\n",
            "Epoch 5/40\n",
            "210/210 [==============================] - 18s 88ms/step - loss: 0.0542 - accuracy: 0.9827\n",
            "Epoch 6/40\n",
            "210/210 [==============================] - 19s 92ms/step - loss: 0.0386 - accuracy: 0.9875\n",
            "Epoch 7/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0315 - accuracy: 0.9901\n",
            "Epoch 8/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0265 - accuracy: 0.9910\n",
            "Epoch 9/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0216 - accuracy: 0.9932\n",
            "Epoch 10/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0204 - accuracy: 0.9936\n",
            "Epoch 11/40\n",
            "210/210 [==============================] - 19s 89ms/step - loss: 0.0182 - accuracy: 0.9942\n",
            "Epoch 12/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0174 - accuracy: 0.9942\n",
            "Epoch 13/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0152 - accuracy: 0.9948\n",
            "Epoch 14/40\n",
            "210/210 [==============================] - 20s 94ms/step - loss: 0.0159 - accuracy: 0.9946\n",
            "Epoch 15/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0158 - accuracy: 0.9944\n",
            "Epoch 16/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0131 - accuracy: 0.9957\n",
            "Epoch 17/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0133 - accuracy: 0.9957\n",
            "Epoch 18/40\n",
            "210/210 [==============================] - 19s 89ms/step - loss: 0.0138 - accuracy: 0.9952\n",
            "Epoch 19/40\n",
            "210/210 [==============================] - 19s 88ms/step - loss: 0.0146 - accuracy: 0.9951\n",
            "Epoch 20/40\n",
            "210/210 [==============================] - 19s 89ms/step - loss: 0.0126 - accuracy: 0.9953\n",
            "Epoch 21/40\n",
            "210/210 [==============================] - 18s 88ms/step - loss: 0.0130 - accuracy: 0.9954\n",
            "Epoch 22/40\n",
            "210/210 [==============================] - 19s 92ms/step - loss: 0.0129 - accuracy: 0.9957\n",
            "Epoch 23/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0123 - accuracy: 0.9955\n",
            "Epoch 24/40\n",
            "210/210 [==============================] - 19s 89ms/step - loss: 0.0128 - accuracy: 0.9958\n",
            "Epoch 25/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0105 - accuracy: 0.9964\n",
            "Epoch 26/40\n",
            "210/210 [==============================] - 19s 92ms/step - loss: 0.0116 - accuracy: 0.9958\n",
            "Epoch 27/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0106 - accuracy: 0.9963\n",
            "Epoch 28/40\n",
            "210/210 [==============================] - 18s 88ms/step - loss: 0.0101 - accuracy: 0.9963\n",
            "Epoch 29/40\n",
            "210/210 [==============================] - 18s 84ms/step - loss: 0.0116 - accuracy: 0.9958\n",
            "Epoch 30/40\n",
            "210/210 [==============================] - 18s 85ms/step - loss: 0.0129 - accuracy: 0.9954\n",
            "Epoch 31/40\n",
            "210/210 [==============================] - 18s 87ms/step - loss: 0.0105 - accuracy: 0.9962\n",
            "Epoch 32/40\n",
            "210/210 [==============================] - 18s 86ms/step - loss: 0.0107 - accuracy: 0.9961\n",
            "Epoch 33/40\n",
            "210/210 [==============================] - 18s 85ms/step - loss: 0.0079 - accuracy: 0.9969\n",
            "Epoch 34/40\n",
            "210/210 [==============================] - 17s 82ms/step - loss: 0.0090 - accuracy: 0.9965\n",
            "Epoch 35/40\n",
            "210/210 [==============================] - 18s 83ms/step - loss: 0.0087 - accuracy: 0.9965\n",
            "Epoch 36/40\n",
            "210/210 [==============================] - 17s 79ms/step - loss: 0.0093 - accuracy: 0.9966\n",
            "Epoch 37/40\n",
            "210/210 [==============================] - 17s 79ms/step - loss: 0.0085 - accuracy: 0.9966\n",
            "Epoch 38/40\n",
            "210/210 [==============================] - 17s 79ms/step - loss: 0.0108 - accuracy: 0.9962\n",
            "Epoch 39/40\n",
            "210/210 [==============================] - 17s 81ms/step - loss: 0.0087 - accuracy: 0.9966\n",
            "Epoch 40/40\n",
            "210/210 [==============================] - 17s 81ms/step - loss: 0.0089 - accuracy: 0.9967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJU-wLZbVLF6",
        "outputId": "6f059eca-c91e-41cc-8637-5c26312baf29"
      },
      "source": [
        "# define the model\r\n",
        "def define_model3(padded_train_sequences, Y_train):\r\n",
        "\t# channel 1\r\n",
        "\tembedding_dim = 150\r\n",
        "\tinputs1 = Input(shape=(MAX_LENGTH,))\r\n",
        "\tembedding1 = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH)(inputs1)\r\n",
        "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n",
        "\tdrop1 = Dropout(0.5)(conv1)\r\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\r\n",
        "\tflat1 = Flatten()(pool1)\r\n",
        "\t# channel 2\r\n",
        "\t#inputs2 = Input(shape=(MAX_LENGTH,))\r\n",
        "\t#embedding2 = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH)(inputs2)\r\n",
        "\t#conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\r\n",
        "\t#drop2 = Dropout(0.5)(conv2)\r\n",
        "\t#pool2 = MaxPooling1D(pool_size=2)(drop2)\r\n",
        "\t#flat2 = Flatten()(pool2)\r\n",
        "\t# channel 3\r\n",
        "\t#inputs3 = Input(shape=(MAX_LENGTH,))\r\n",
        "\t#embedding3 = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH)(inputs3)\r\n",
        "\t#conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\r\n",
        "\t#drop3 = Dropout(0.5)(conv3)\r\n",
        "\t#pool3 = MaxPooling1D(pool_size=2)(drop3)\r\n",
        "\t#flat3 = Flatten()(pool3)\r\n",
        "\t# merge\r\n",
        "\t#merged = concatenate([flat1, flat2, flat3])\r\n",
        "\t# interpretation\r\n",
        "\tdense1 = Dense(20, activation='relu')(flat1)\r\n",
        "\toutputs = Dense(2, activation='sigmoid')(dense1)\r\n",
        "\tmodel = Model(inputs=inputs1, outputs=outputs)\r\n",
        "\t# compile\r\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\tmodel.fit([padded_train_sequences], Y_train, epochs=40, batch_size=200)\r\n",
        "\r\n",
        "\t#print(model.summary())\r\n",
        "\treturn model\r\n",
        "\r\n",
        "model = define_model3(padded_train_sequences, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "210/210 [==============================] - 21s 96ms/step - loss: 0.5661 - accuracy: 0.6628\n",
            "Epoch 2/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.2393 - accuracy: 0.9071\n",
            "Epoch 3/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.1452 - accuracy: 0.9475\n",
            "Epoch 4/40\n",
            "210/210 [==============================] - 20s 93ms/step - loss: 0.0934 - accuracy: 0.9680\n",
            "Epoch 5/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0605 - accuracy: 0.9800\n",
            "Epoch 6/40\n",
            "210/210 [==============================] - 19s 92ms/step - loss: 0.0407 - accuracy: 0.9862\n",
            "Epoch 7/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0343 - accuracy: 0.9886\n",
            "Epoch 8/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0296 - accuracy: 0.9901\n",
            "Epoch 9/40\n",
            "210/210 [==============================] - 18s 87ms/step - loss: 0.0233 - accuracy: 0.9919\n",
            "Epoch 10/40\n",
            "210/210 [==============================] - 19s 88ms/step - loss: 0.0235 - accuracy: 0.9922\n",
            "Epoch 11/40\n",
            "210/210 [==============================] - 18s 87ms/step - loss: 0.0195 - accuracy: 0.9932\n",
            "Epoch 12/40\n",
            "210/210 [==============================] - 19s 88ms/step - loss: 0.0199 - accuracy: 0.9933\n",
            "Epoch 13/40\n",
            "210/210 [==============================] - 18s 87ms/step - loss: 0.0187 - accuracy: 0.9939\n",
            "Epoch 14/40\n",
            "210/210 [==============================] - 18s 87ms/step - loss: 0.0184 - accuracy: 0.9930\n",
            "Epoch 15/40\n",
            "210/210 [==============================] - 18s 87ms/step - loss: 0.0169 - accuracy: 0.9939\n",
            "Epoch 16/40\n",
            "210/210 [==============================] - 19s 88ms/step - loss: 0.0163 - accuracy: 0.9945\n",
            "Epoch 17/40\n",
            "210/210 [==============================] - 19s 89ms/step - loss: 0.0137 - accuracy: 0.9954\n",
            "Epoch 18/40\n",
            "210/210 [==============================] - 20s 96ms/step - loss: 0.0143 - accuracy: 0.9948\n",
            "Epoch 19/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0134 - accuracy: 0.9953\n",
            "Epoch 20/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0149 - accuracy: 0.9944\n",
            "Epoch 21/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0155 - accuracy: 0.9946\n",
            "Epoch 22/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0155 - accuracy: 0.9947\n",
            "Epoch 23/40\n",
            "210/210 [==============================] - 20s 94ms/step - loss: 0.0139 - accuracy: 0.9945\n",
            "Epoch 24/40\n",
            "210/210 [==============================] - 20s 97ms/step - loss: 0.0117 - accuracy: 0.9958\n",
            "Epoch 25/40\n",
            "210/210 [==============================] - 19s 89ms/step - loss: 0.0116 - accuracy: 0.9956\n",
            "Epoch 26/40\n",
            "210/210 [==============================] - 19s 89ms/step - loss: 0.0123 - accuracy: 0.9957\n",
            "Epoch 27/40\n",
            "210/210 [==============================] - 19s 89ms/step - loss: 0.0103 - accuracy: 0.9961\n",
            "Epoch 28/40\n",
            "210/210 [==============================] - 18s 85ms/step - loss: 0.0096 - accuracy: 0.9961\n",
            "Epoch 29/40\n",
            "210/210 [==============================] - 19s 89ms/step - loss: 0.0099 - accuracy: 0.9960\n",
            "Epoch 30/40\n",
            "210/210 [==============================] - 18s 87ms/step - loss: 0.0122 - accuracy: 0.9955\n",
            "Epoch 31/40\n",
            "210/210 [==============================] - 18s 88ms/step - loss: 0.0112 - accuracy: 0.9956\n",
            "Epoch 32/40\n",
            "210/210 [==============================] - 18s 87ms/step - loss: 0.0092 - accuracy: 0.9965\n",
            "Epoch 33/40\n",
            "210/210 [==============================] - 19s 89ms/step - loss: 0.0112 - accuracy: 0.9967\n",
            "Epoch 34/40\n",
            "210/210 [==============================] - 21s 99ms/step - loss: 0.0095 - accuracy: 0.9965\n",
            "Epoch 35/40\n",
            "210/210 [==============================] - 19s 92ms/step - loss: 0.0089 - accuracy: 0.9961\n",
            "Epoch 36/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0082 - accuracy: 0.9968\n",
            "Epoch 37/40\n",
            "210/210 [==============================] - 19s 91ms/step - loss: 0.0085 - accuracy: 0.9968\n",
            "Epoch 38/40\n",
            "210/210 [==============================] - 20s 93ms/step - loss: 0.0098 - accuracy: 0.9965\n",
            "Epoch 39/40\n",
            "210/210 [==============================] - 19s 93ms/step - loss: 0.0090 - accuracy: 0.9965\n",
            "Epoch 40/40\n",
            "210/210 [==============================] - 19s 90ms/step - loss: 0.0102 - accuracy: 0.9957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeOOo0HgafAy",
        "outputId": "7a0152eb-3206-4a6a-ffbd-6c973a5553d2"
      },
      "source": [
        "\r\n",
        "#accuracyscore = model.evaluate([padded_test_sequences,padded_test_sequences, padded_test_sequences], Y_test)\r\n",
        "accuracyscore = model.evaluate( padded_test_sequences, Y_test)\r\n",
        "\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "\r\n",
        "print(accuracyscore * 100)\r\n",
        "print(\"Test loss:\", accuracyscore[0])\r\n",
        "print(\"Test accuracy:\", accuracyscore[1] * 100)\r\n",
        "\r\n",
        "y_pred = model.predict([padded_test_sequences])\r\n",
        "\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "print(confusion_matrix(y_test, np.argmax(y_pred, axis = 1)))\r\n",
        "print(classification_report(y_test, np.argmax(y_pred, axis = 1)))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "436/436 [==============================] - 2s 4ms/step - loss: 1.0764 - accuracy: 0.8728\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 343)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 343, 150)          12000000  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 340, 32)           19232     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 340, 32)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 170, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 5440)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 20)                108820    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 42        \n",
            "=================================================================\n",
            "Total params: 12,128,094\n",
            "Trainable params: 12,128,094\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "[1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746, 1.0763790607452393, 0.8728041648864746]\n",
            "Test loss: 1.0763790607452393\n",
            "Test accuracy: 87.28041648864746\n",
            "[[5063 1069]\n",
            " [ 705 7110]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.83      0.85      6132\n",
            "           1       0.87      0.91      0.89      7815\n",
            "\n",
            "    accuracy                           0.87     13947\n",
            "   macro avg       0.87      0.87      0.87     13947\n",
            "weighted avg       0.87      0.87      0.87     13947\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IcCMqSQfwJG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_mHxJZdb-TJ",
        "outputId": "41e3fcaa-7c95-4271-915e-998dc2cbe9d4"
      },
      "source": [
        "# define the model\r\n",
        "def define_Ch3model(padded_train_sequences, y_train):\r\n",
        "\t# channel 1\r\n",
        "  embed_dim = 40  # Embedding size for each token\r\n",
        "  num_heads = 4  # Number of attention heads\r\n",
        "  ff_dim = 32  # Hidden layer size in feed forward network inside transformer\r\n",
        "  maxlen = MAX_SEQUENCE_LENGTH\r\n",
        "  vocab_size = vocab_size = len(tokenizer.word_index) + 1 #80000\r\n",
        "  print(vocab_size)\r\n",
        "  inputs1 = Input(shape=(maxlen,))\r\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\n",
        "  x1 = embedding_layer(inputs1)\r\n",
        "  transformer_block = TransformerBlock(embed_dim, 10, ff_dim)\r\n",
        "  x1 = transformer_block(x1)\r\n",
        "  x1=  MaxPooling1D()(x1)\r\n",
        "  x1 = Dense(30, activation=\"relu\")(x1)\r\n",
        "  x1 = Dropout(0.2)(x1)\r\n",
        "  x1 = Flatten()(x1)\r\n",
        "\t# channel 2\r\n",
        "  inputs2 = Input(shape=(MAX_LENGTH,))\r\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\n",
        "  x2 = embedding_layer(inputs2)\r\n",
        "  transformer_block = TransformerBlock(embed_dim, 10, ff_dim)\r\n",
        "  x2 = transformer_block(x2)\r\n",
        "  x2= MaxPooling1D()(x2)\r\n",
        "  x2 = Dense(30, activation=\"relu\")(x2)\r\n",
        "  x2 = Dropout(0.2)(x2)\r\n",
        "  x2 = Flatten()(x2)\r\n",
        "\t# channel 3\r\n",
        "  #inputs3 = Input(shape=(MAX_LENGTH,))\r\n",
        "  #embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\n",
        "  #x3 = embedding_layer(inputs3)\r\n",
        "  #transformer_block = TransformerBlock(embed_dim, 10, ff_dim)\r\n",
        "  #x3 = transformer_block(x3)\r\n",
        "  #x3= MaxPooling1D()(x3)\r\n",
        "  #x3 = Dense(30, activation=\"relu\")(x3)\r\n",
        "  #x3 = Dropout(0.1)(x3)\r\n",
        "  #x3 = Flatten()(x3)\r\n",
        "  embedding_dim = 150\r\n",
        "  inputs3 = Input(shape=(MAX_LENGTH,))\r\n",
        "  embedding1 = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH)(inputs3)\r\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n",
        "  drop1 = Dropout(0.5)(conv1)\r\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\r\n",
        "  x3 = Flatten()(pool1)\r\n",
        "\t# merge\r\n",
        "  merged = concatenate([x1, x2, x3])\r\n",
        "  #merged1 = Flatten()(merged1)\r\n",
        "\t# interpretation\r\n",
        "\r\n",
        "  #merged = Conv1D(filters=(3), kernel_size=(20), activation='relu')(merged1)\r\n",
        "  dense1 = Dense(60, activation='relu')(merged)\r\n",
        "  dense2 = Dense(30, activation='relu')(dense1)\r\n",
        "  outputs = Dense(2, activation='softmax')(dense2)\r\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\r\n",
        "  \r\n",
        "\t# compile\r\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "  #model.fit([padded_train_sequences,padded_train_sequences,padded_train_sequences], y_train, epochs=3, batch_size=16)\r\n",
        "  history = model.fit(x=[padded_train_sequences,padded_train_sequences, padded_train_sequences], y=Y_train, \r\n",
        "                    #validation_data=(padded_test_sequences, Y_test), \r\n",
        "                    batch_size=200, \r\n",
        "                    #callbacks=[checkpoint], \r\n",
        "                    epochs=40, \r\n",
        "                    verbose=1)\r\n",
        "  print(model.summary())\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "model = define_Ch3model(padded_train_sequences, Y_train)\r\n",
        "#model = define_model3(padded_train_sequences, Y_train)\r\n",
        "#define_model3\r\n",
        "\r\n",
        "accuracyscore = model.evaluate([padded_test_sequences,padded_test_sequences, padded_test_sequences], Y_test)\r\n",
        "#accuracyscore = model.evaluate( padded_test_sequences, Y_test)\r\n",
        "\r\n",
        "print(accuracyscore * 100)\r\n",
        "print(\"Test loss:\", accuracyscore[0])\r\n",
        "print(\"Test accuracy:\", accuracyscore[1] * 100)\r\n",
        "\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "print(confusion_matrix(y_test, np.argmax(y_pred, axis = 1)))\r\n",
        "print(classification_report(y_test, np.argmax(y_pred, axis = 1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46825\n",
            "Epoch 1/40\n",
            "210/210 [==============================] - 519s 2s/step - loss: 0.6080 - accuracy: 0.6466\n",
            "Epoch 2/40\n",
            "210/210 [==============================] - 472s 2s/step - loss: 0.2383 - accuracy: 0.9065\n",
            "Epoch 3/40\n",
            "210/210 [==============================] - 445s 2s/step - loss: 0.1071 - accuracy: 0.9640\n",
            "Epoch 4/40\n",
            "210/210 [==============================] - 430s 2s/step - loss: 0.0554 - accuracy: 0.9823\n",
            "Epoch 5/40\n",
            "210/210 [==============================] - 420s 2s/step - loss: 0.0345 - accuracy: 0.9898\n",
            "Epoch 6/40\n",
            "210/210 [==============================] - 439s 2s/step - loss: 0.0299 - accuracy: 0.9916\n",
            "Epoch 7/40\n",
            "210/210 [==============================] - 425s 2s/step - loss: 0.0227 - accuracy: 0.9932\n",
            "Epoch 8/40\n",
            "210/210 [==============================] - 419s 2s/step - loss: 0.0191 - accuracy: 0.9947\n",
            "Epoch 9/40\n",
            "210/210 [==============================] - 424s 2s/step - loss: 0.0165 - accuracy: 0.9950\n",
            "Epoch 10/40\n",
            "210/210 [==============================] - 438s 2s/step - loss: 0.0160 - accuracy: 0.9949\n",
            "Epoch 11/40\n",
            "210/210 [==============================] - 431s 2s/step - loss: 0.0135 - accuracy: 0.9954\n",
            "Epoch 12/40\n",
            "210/210 [==============================] - 385s 2s/step - loss: 0.0130 - accuracy: 0.9957\n",
            "Epoch 13/40\n",
            "210/210 [==============================] - 378s 2s/step - loss: 0.0114 - accuracy: 0.9960\n",
            "Epoch 14/40\n",
            "210/210 [==============================] - 384s 2s/step - loss: 0.0129 - accuracy: 0.9954\n",
            "Epoch 15/40\n",
            "210/210 [==============================] - 380s 2s/step - loss: 0.0113 - accuracy: 0.9960\n",
            "Epoch 16/40\n",
            "210/210 [==============================] - 374s 2s/step - loss: 0.0115 - accuracy: 0.9952\n",
            "Epoch 17/40\n",
            "210/210 [==============================] - 379s 2s/step - loss: 0.0093 - accuracy: 0.9963\n",
            "Epoch 18/40\n",
            "210/210 [==============================] - 374s 2s/step - loss: 0.0107 - accuracy: 0.9961\n",
            "Epoch 19/40\n",
            "210/210 [==============================] - 376s 2s/step - loss: 0.0096 - accuracy: 0.9963\n",
            "Epoch 20/40\n",
            "210/210 [==============================] - 379s 2s/step - loss: 0.0090 - accuracy: 0.9965\n",
            "Epoch 21/40\n",
            "210/210 [==============================] - 381s 2s/step - loss: 0.0086 - accuracy: 0.9962\n",
            "Epoch 22/40\n",
            "210/210 [==============================] - 379s 2s/step - loss: 0.0090 - accuracy: 0.9963\n",
            "Epoch 23/40\n",
            "210/210 [==============================] - 380s 2s/step - loss: 0.0098 - accuracy: 0.9958\n",
            "Epoch 24/40\n",
            "210/210 [==============================] - 380s 2s/step - loss: 0.0099 - accuracy: 0.9957\n",
            "Epoch 25/40\n",
            "210/210 [==============================] - 381s 2s/step - loss: 0.0105 - accuracy: 0.9961\n",
            "Epoch 26/40\n",
            "210/210 [==============================] - 375s 2s/step - loss: 0.0084 - accuracy: 0.9967\n",
            "Epoch 27/40\n",
            " 48/210 [=====>........................] - ETA: 4:52 - loss: 0.0064 - accuracy: 0.9972"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhIGBejNdSwr",
        "outputId": "25086e23-203a-4cd7-fe88-3a98de5bf9a3"
      },
      "source": [
        "model.save('content/saved_model/model_last')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_38_layer_call_fn, embedding_38_layer_call_and_return_conditional_losses, embedding_39_layer_call_fn, embedding_39_layer_call_and_return_conditional_losses, embedding_40_layer_call_fn while saving (showing 5 of 110). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as embedding_38_layer_call_fn, embedding_38_layer_call_and_return_conditional_losses, embedding_39_layer_call_fn, embedding_39_layer_call_and_return_conditional_losses, embedding_40_layer_call_fn while saving (showing 5 of 110). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/saved_model/model_last/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: content/saved_model/model_last/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5ISbxlEdjqC"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aWhN7BIeGrs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZETHqYnleHv6"
      },
      "source": [
        "model.save_weights('content/saved_model/model_lastW') \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irZnXxg4fxSY"
      },
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "y_pred = model.predict([padded_test_sequences,padded_test_sequences, padded_test_sequences])\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtYeQcgSmdKx",
        "outputId": "5a045e83-5bbf-44c7-c68b-9ec92014b0a2"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "print(confusion_matrix(y_test, np.argmax(y_pred, axis = 1)))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8673  471]\n",
            " [1036 8768]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXpQKWFYov8c",
        "outputId": "4a8e3dca-7067-48f1-fba9-7806074a3bbb"
      },
      "source": [
        "print(classification_report(y_test, np.argmax(y_pred, axis = 1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.95      0.92      9144\n",
            "           1       0.95      0.89      0.92      9804\n",
            "\n",
            "    accuracy                           0.92     18948\n",
            "   macro avg       0.92      0.92      0.92     18948\n",
            "weighted avg       0.92      0.92      0.92     18948\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM42LYeWkX_R",
        "outputId": "6a51dfb7-a04c-469a-bdb2-9085eabd7676"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.9999952e-01, 4.5035904e-07],\n",
              "       [1.1805972e-21, 1.0000000e+00],\n",
              "       [1.0000000e+00, 1.4593946e-37],\n",
              "       ...,\n",
              "       [1.0000000e+00, 8.3060545e-24],\n",
              "       [8.8793976e-25, 1.0000000e+00],\n",
              "       [2.2954686e-14, 1.0000000e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX-jVLNqi6-M",
        "outputId": "23468a37-e835-4a73-8127-b63de71044e6"
      },
      "source": [
        "# define the model\r\n",
        "def define_Ch3model(padded_train_sequences, y_train):\r\n",
        "\t# channel 1\r\n",
        "  \r\n",
        "  embed_dim = 40  # Embedding size for each token\r\n",
        "  embedding_matrix = np.random.random((MAX_NB_WORDS, embed_dim))\r\n",
        "  num_heads = 4  # Number of attention heads\r\n",
        "  ff_dim = 32  # Hidden layer size in feed forward network inside transformer\r\n",
        "  \r\n",
        "  maxlen = MAX_SEQUENCE_LENGTH\r\n",
        "  vocab_size = vocab_size = len(tokenizer.word_index) + 1 #80000\r\n",
        "  print(vocab_size)\r\n",
        "  inputs1 = Input(shape=(maxlen,))\r\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\n",
        "  x1 = embedding_layer(inputs1)\r\n",
        "  transformer_block = TransformerBlock(embed_dim, 10, ff_dim)\r\n",
        "  x1 = transformer_block(x1)\r\n",
        "  x1=  MaxPooling1D()(x1)\r\n",
        "  #x1 = Dense(30, activation=\"relu\")(x1)\r\n",
        "  x1 = Dropout(0.5)(x1)\r\n",
        "  x1 = Flatten()(x1)\r\n",
        "\t# channel 2\r\n",
        "  \r\n",
        "  inputs2 = Input(shape=(MAX_LENGTH, ))\r\n",
        "  x2 = Embedding(input_dim=MAX_NB_WORDS, output_dim=embed_dim, input_length=maxlen, \r\n",
        "                weights=[embedding_matrix], trainable=True)(inputs2)\r\n",
        "  x2 = SpatialDropout1D(0.1)(x2)\r\n",
        "  x2 = Bidirectional(GRU(10, return_sequences=True))(x2)\r\n",
        "  avg_pool = GlobalAveragePooling1D()(x2)\r\n",
        "  max_pool = GlobalMaxPooling1D()(x2)\r\n",
        "  x2 = concatenate([avg_pool, max_pool])\r\n",
        "\r\n",
        "\r\n",
        "  #inputs2 = Input(shape=(MAX_LENGTH,))\r\n",
        "  #embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\n",
        "  #x2 = embedding_layer(inputs2)\r\n",
        "  #transformer_block = TransformerBlock(embed_dim, 10, ff_dim)\r\n",
        "  #x2 = transformer_block(x2)\r\n",
        "  #x2= MaxPooling1D()(x2)\r\n",
        "  #x2 = Dense(30, activation=\"relu\")(x2)\r\n",
        "  #x2 = Dropout(0.2)(x2)\r\n",
        "  #x2 = Flatten()(x2)\r\n",
        "\t# channel 3\r\n",
        "  #inputs3 = Input(shape=(MAX_LENGTH,))\r\n",
        "  #embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\n",
        "  #x3 = embedding_layer(inputs3)\r\n",
        "  #transformer_block = TransformerBlock(embed_dim, 10, ff_dim)\r\n",
        "  #x3 = transformer_block(x3)\r\n",
        "  #x3= MaxPooling1D()(x3)\r\n",
        "  #x3 = Dense(30, activation=\"relu\")(x3)\r\n",
        "  #x3 = Dropout(0.1)(x3)\r\n",
        "  #x3 = Flatten()(x3)\r\n",
        "  embedding_dim = 150\r\n",
        "  inputs3 = Input(shape=(MAX_LENGTH,))\r\n",
        "  embedding1 = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH)(inputs3)\r\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n",
        "  drop1 = Dropout(0.5)(conv1)\r\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\r\n",
        "  x3 = Flatten()(pool1)\r\n",
        "\t# merge\r\n",
        "  merged = concatenate([x1, x2, x3])\r\n",
        "  #merged1 = Flatten()(merged1)\r\n",
        "\t# interpretation\r\n",
        "\r\n",
        "  #merged = Conv1D(filters=(3), kernel_size=(20), activation='relu')(merged1)\r\n",
        "  dense1 = Dense(60, activation='relu')(merged)\r\n",
        "  dense2 = Dense(30, activation='relu')(dense1)\r\n",
        "  outputs = Dense(2, activation='softmax')(dense2)\r\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\r\n",
        "  \r\n",
        "\t# compile\r\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "  #model.fit([padded_train_sequences,padded_train_sequences,padded_train_sequences], y_train, epochs=3, batch_size=16)\r\n",
        "  history = model.fit(x=[padded_train_sequences,padded_train_sequences, padded_train_sequences], y=Y_train, \r\n",
        "                    #validation_data=(padded_test_sequences, Y_test), \r\n",
        "                    batch_size=100, \r\n",
        "                    #callbacks=[checkpoint], \r\n",
        "                    epochs=50, \r\n",
        "                    verbose=1)\r\n",
        "  print(model.summary())\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "model = define_Ch3model(padded_train_sequences, Y_train)\r\n",
        "#model = define_model3(padded_train_sequences, Y_train)\r\n",
        "#define_model3\r\n",
        "\r\n",
        "accuracyscore = model.evaluate([padded_test_sequences,padded_test_sequences, padded_test_sequences], Y_test)\r\n",
        "#accuracyscore = model.evaluate( padded_test_sequences, Y_test)\r\n",
        "\r\n",
        "print(accuracyscore * 100)\r\n",
        "print(\"Test loss:\", accuracyscore[0])\r\n",
        "print(\"Test accuracy:\", accuracyscore[1] * 100)\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "y_pred = model.predict([padded_test_sequences,padded_test_sequences, padded_test_sequences])\r\n",
        "\r\n",
        "print(confusion_matrix(y_test, np.argmax(y_pred, axis = 1)))\r\n",
        "print(classification_report(y_test, np.argmax(y_pred, axis = 1)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47036\n",
            "Epoch 1/50\n",
            "419/419 [==============================] - 376s 886ms/step - loss: 0.6011 - accuracy: 0.6551\n",
            "Epoch 2/50\n",
            "419/419 [==============================] - 404s 964ms/step - loss: 0.2495 - accuracy: 0.9011\n",
            "Epoch 3/50\n",
            "419/419 [==============================] - 393s 938ms/step - loss: 0.1500 - accuracy: 0.9446\n",
            "Epoch 4/50\n",
            "419/419 [==============================] - 403s 961ms/step - loss: 0.0862 - accuracy: 0.9682\n",
            "Epoch 5/50\n",
            "419/419 [==============================] - 394s 940ms/step - loss: 0.0593 - accuracy: 0.9809\n",
            "Epoch 6/50\n",
            "419/419 [==============================] - 379s 903ms/step - loss: 0.0427 - accuracy: 0.9859\n",
            "Epoch 7/50\n",
            "419/419 [==============================] - 393s 938ms/step - loss: 0.0371 - accuracy: 0.9878\n",
            "Epoch 8/50\n",
            "419/419 [==============================] - 391s 934ms/step - loss: 0.0291 - accuracy: 0.9902\n",
            "Epoch 9/50\n",
            "419/419 [==============================] - 414s 989ms/step - loss: 0.0237 - accuracy: 0.9920\n",
            "Epoch 10/50\n",
            "419/419 [==============================] - 369s 881ms/step - loss: 0.0218 - accuracy: 0.9928\n",
            "Epoch 11/50\n",
            "419/419 [==============================] - 370s 883ms/step - loss: 0.0186 - accuracy: 0.9936\n",
            "Epoch 12/50\n",
            "419/419 [==============================] - 370s 884ms/step - loss: 0.0165 - accuracy: 0.9946\n",
            "Epoch 13/50\n",
            "419/419 [==============================] - 366s 874ms/step - loss: 0.0156 - accuracy: 0.9953\n",
            "Epoch 14/50\n",
            "419/419 [==============================] - 376s 896ms/step - loss: 0.0147 - accuracy: 0.9947\n",
            "Epoch 15/50\n",
            "419/419 [==============================] - 370s 884ms/step - loss: 0.0161 - accuracy: 0.9944\n",
            "Epoch 16/50\n",
            "419/419 [==============================] - 373s 891ms/step - loss: 0.0151 - accuracy: 0.9950\n",
            "Epoch 17/50\n",
            "419/419 [==============================] - 379s 905ms/step - loss: 0.0143 - accuracy: 0.9949\n",
            "Epoch 18/50\n",
            "419/419 [==============================] - 378s 902ms/step - loss: 0.0125 - accuracy: 0.9957\n",
            "Epoch 19/50\n",
            "419/419 [==============================] - 367s 877ms/step - loss: 0.0121 - accuracy: 0.9961\n",
            "Epoch 20/50\n",
            "419/419 [==============================] - 370s 882ms/step - loss: 0.0114 - accuracy: 0.9961\n",
            "Epoch 21/50\n",
            "419/419 [==============================] - 368s 878ms/step - loss: 0.0108 - accuracy: 0.9961\n",
            "Epoch 22/50\n",
            "419/419 [==============================] - 367s 876ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 23/50\n",
            "419/419 [==============================] - 368s 878ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 24/50\n",
            "419/419 [==============================] - 380s 908ms/step - loss: 0.0090 - accuracy: 0.9968\n",
            "Epoch 25/50\n",
            "419/419 [==============================] - 379s 904ms/step - loss: 0.0134 - accuracy: 0.9959\n",
            "Epoch 26/50\n",
            "419/419 [==============================] - 385s 918ms/step - loss: 0.0096 - accuracy: 0.9965\n",
            "Epoch 27/50\n",
            "419/419 [==============================] - 367s 877ms/step - loss: 0.0075 - accuracy: 0.9972\n",
            "Epoch 28/50\n",
            "419/419 [==============================] - 373s 889ms/step - loss: 0.0090 - accuracy: 0.9974\n",
            "Epoch 29/50\n",
            "419/419 [==============================] - 368s 878ms/step - loss: 0.0088 - accuracy: 0.9968\n",
            "Epoch 30/50\n",
            "419/419 [==============================] - 365s 870ms/step - loss: 0.0078 - accuracy: 0.9973\n",
            "Epoch 31/50\n",
            "419/419 [==============================] - 365s 870ms/step - loss: 0.0085 - accuracy: 0.9967\n",
            "Epoch 32/50\n",
            "419/419 [==============================] - 385s 918ms/step - loss: 0.0077 - accuracy: 0.9972\n",
            "Epoch 33/50\n",
            "419/419 [==============================] - 381s 910ms/step - loss: 0.0079 - accuracy: 0.9973\n",
            "Epoch 34/50\n",
            "419/419 [==============================] - 385s 919ms/step - loss: 0.0085 - accuracy: 0.9971\n",
            "Epoch 35/50\n",
            "419/419 [==============================] - 394s 941ms/step - loss: 0.0076 - accuracy: 0.9970\n",
            "Epoch 36/50\n",
            "419/419 [==============================] - 389s 928ms/step - loss: 0.0069 - accuracy: 0.9975\n",
            "Epoch 37/50\n",
            "419/419 [==============================] - 392s 936ms/step - loss: 0.0067 - accuracy: 0.9972\n",
            "Epoch 38/50\n",
            "419/419 [==============================] - 401s 957ms/step - loss: 0.0072 - accuracy: 0.9971\n",
            "Epoch 39/50\n",
            "419/419 [==============================] - 407s 971ms/step - loss: 0.0075 - accuracy: 0.9967\n",
            "Epoch 40/50\n",
            "419/419 [==============================] - 407s 971ms/step - loss: 0.0071 - accuracy: 0.9970\n",
            "Epoch 41/50\n",
            "419/419 [==============================] - 400s 954ms/step - loss: 0.0073 - accuracy: 0.9969\n",
            "Epoch 42/50\n",
            "419/419 [==============================] - 392s 936ms/step - loss: 0.0073 - accuracy: 0.9969\n",
            "Epoch 43/50\n",
            "419/419 [==============================] - 382s 911ms/step - loss: 0.0073 - accuracy: 0.9971\n",
            "Epoch 44/50\n",
            "419/419 [==============================] - 364s 869ms/step - loss: 0.0061 - accuracy: 0.9975\n",
            "Epoch 45/50\n",
            "419/419 [==============================] - 365s 871ms/step - loss: 0.0070 - accuracy: 0.9968\n",
            "Epoch 46/50\n",
            "419/419 [==============================] - 365s 871ms/step - loss: 0.0058 - accuracy: 0.9977\n",
            "Epoch 47/50\n",
            "419/419 [==============================] - 365s 870ms/step - loss: 0.0057 - accuracy: 0.9976\n",
            "Epoch 48/50\n",
            "419/419 [==============================] - 363s 866ms/step - loss: 0.0058 - accuracy: 0.9977\n",
            "Epoch 49/50\n",
            "419/419 [==============================] - 364s 868ms/step - loss: 0.0058 - accuracy: 0.9979\n",
            "Epoch 50/50\n",
            "419/419 [==============================] - 366s 873ms/step - loss: 0.0058 - accuracy: 0.9975\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 343)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 343)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 343)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_and_position_embedding_1  (None, 343, 40)      1895160     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 343, 40)      3200000     input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 343, 150)     12000000    input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block_1 (Transforme (None, 343, 40)      9352        token_and_position_embedding_1[0]\n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1 (SpatialDro (None, 343, 40)      0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 340, 32)      19232       embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 171, 40)      0           transformer_block_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 343, 20)      3120        spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 340, 32)      0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 171, 40)      0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 20)           0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 20)           0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 170, 32)      0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 6840)         0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 40)           0           global_average_pooling1d_1[0][0] \n",
            "                                                                 global_max_pooling1d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 5440)         0           max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 12320)        0           flatten_2[0][0]                  \n",
            "                                                                 concatenate_2[0][0]              \n",
            "                                                                 flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 60)           739260      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 30)           1830        dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 2)            62          dense_16[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 17,868,016\n",
            "Trainable params: 17,868,016\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "436/436 [==============================] - 44s 97ms/step - loss: 1.0210 - accuracy: 0.8800\n",
            "[1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407]\n",
            "Test loss: 1.0209559202194214\n",
            "Test accuracy: 87.99741864204407\n",
            "[[5188  913]\n",
            " [ 761 7085]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86      6101\n",
            "           1       0.89      0.90      0.89      7846\n",
            "\n",
            "    accuracy                           0.88     13947\n",
            "   macro avg       0.88      0.88      0.88     13947\n",
            "weighted avg       0.88      0.88      0.88     13947\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh76_93BqHTi"
      },
      "source": [
        "Total params: 17,868,016\r\n",
        "Trainable params: 17,868,016\r\n",
        "Non-trainable params: 0\r\n",
        "__________________________________________________________________________________________________\r\n",
        "None\r\n",
        "436/436 [==============================] - 44s 97ms/step - loss: 1.0210 - accuracy: 0.8800\r\n",
        "[1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407, 1.0209559202194214, 0.8799741864204407]\r\n",
        "Test loss: 1.0209559202194214\r\n",
        "Test accuracy: 87.99741864204407\r\n",
        "[[5188  913]\r\n",
        " [ 761 7085]]\r\n",
        "              precision    recall  f1-score   support\r\n",
        "\r\n",
        "           0       0.87      0.85      0.86      6101\r\n",
        "           1       0.89      0.90      0.89      7846\r\n",
        "\r\n",
        "    accuracy                           0.88     13947\r\n",
        "   macro avg       0.88      0.88      0.88     13947\r\n",
        "weighted avg       0.88      0.88      0.88     13947\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCxDqWRPs3hi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEn9XDV3fkbS",
        "outputId": "14197df3-5ef5-42cd-caab-8d537976300f"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "y_pred = model.predict([padded_test_sequences,padded_test_sequences, padded_test_sequences])\r\n",
        "\r\n",
        "print(confusion_matrix(y_test, np.argmax(y_pred, axis = 1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14820  3520]\n",
            " [ 3983 19518]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaZ566Ubg0Zh",
        "outputId": "3d2db394-d622-4fee-b453-612f427b9d7a"
      },
      "source": [
        "print(14820  + 3520)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18340\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "3kGlcEITg8LN",
        "outputId": "6815521a-b5f3-4c88-f86f-4a56ac8f9fac"
      },
      "source": [
        "from collections import defaultdict\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "from yellowbrick.text import FreqDistVisualizer\r\n",
        "#from yellowbrick.datasets import load_hobbies\r\n",
        "\r\n",
        "# Load the text data\r\n",
        "corpus = data\r\n",
        "\r\n",
        "# Create a dict to map target labels to documents of that category\r\n",
        "hobbies = defaultdict(list)\r\n",
        "for text, label in zip(data.full_text, data.label):\r\n",
        "    hobbies[label].append(text)\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(stop_words='english')\r\n",
        "docs  = vectorizer.fit_transform(text for text in hobbies['gaming'])\r\n",
        "features  = vectorizer.get_feature_names()\r\n",
        "\r\n",
        "visualizer = FreqDistVisualizer(\r\n",
        "    features=features, size=(1080, 720)\r\n",
        ")\r\n",
        "visualizer.fit(docs)\r\n",
        "visualizer.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a069437ed2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdocs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhobbies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gaming'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mfeatures\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1WSjYAPhKNF"
      },
      "source": [
        "def pipeline(learner_list, X_train, y_train, X_test, y_test): \r\n",
        "    '''\r\n",
        "    inputs:\r\n",
        "       - learner: the learning algorithm to be trained and predicted on\r\n",
        "       - X_train: features training set\r\n",
        "       - y_train: income training set\r\n",
        "       - X_test: features testing set\r\n",
        "       - y_test: income testing set\r\n",
        "    '''\r\n",
        "    \r\n",
        "    # Get length of Training Data:\r\n",
        "    size = len(y_train)\r\n",
        "    \r\n",
        "    results = {}\r\n",
        "    final_results = []\r\n",
        "    \r\n",
        "    for learner in learner_list:\r\n",
        "        \r\n",
        "        # Store the learner name:\r\n",
        "        results['Algorithm'] = learner.__class__.__name__\r\n",
        "\r\n",
        "        # Fit the learner:\r\n",
        "        start = time() # Get start time\r\n",
        "        print(\"Training {}\".format(learner.__class__.__name__))\r\n",
        "        learner = learner.fit(X_train, y_train)\r\n",
        "        end = time() # Get end time\r\n",
        "\r\n",
        "        # Store the training time\r\n",
        "        results['Training Time'] = end - start\r\n",
        "\r\n",
        "        start = time() # Get start time\r\n",
        "        predictions_test = learner.predict(X_test)\r\n",
        "        predictions_train = learner.predict(X_train)\r\n",
        "        end = time() # Get end time\r\n",
        "\r\n",
        "        # Store the prediction time\r\n",
        "        results['Prediction Time'] = end - start\r\n",
        "\r\n",
        "        # Compute the Accuracy on Test Set\r\n",
        "        results['Accuracy: Test'] = accuracy_score(y_test, predictions_test)\r\n",
        "        \r\n",
        "        # Compute the Accuracy on Training Set\r\n",
        "        results['Accuracy: Train'] = accuracy_score(y_train, predictions_train)\r\n",
        "\r\n",
        "        # Compute the F1 Score on Test Set\r\n",
        "        results['F1 Score: Test'] = f1_score(y_test, predictions_test)\r\n",
        "\r\n",
        "        # Compute the F1 Score on Training Set\r\n",
        "        results['F1 Score: Train'] = f1_score(y_train, predictions_train)\r\n",
        "\r\n",
        "        # Compute the Precision on Test Set\r\n",
        "        results['Precision: Test'] = precision_score(y_test, predictions_test)\r\n",
        "\r\n",
        "        # Compute the Precision on Training Set\r\n",
        "        results['Precision: Train'] = precision_score(y_train, predictions_train)\r\n",
        "\r\n",
        "        # Compute the Recall on Test Set\r\n",
        "        results['Recall: Test'] = recall_score(y_test, predictions_test)\r\n",
        "\r\n",
        "        # Compute the Recall on Training Set\r\n",
        "        results['Recall: Train'] = recall_score(y_train, predictions_train)\r\n",
        "\r\n",
        "        # Success\r\n",
        "        print(\"Training {} finished in {:.2f} sec\".format(learner.__class__.__name__, results['Training Time']))\r\n",
        "        print('----------------------------------------------------')\r\n",
        "        \r\n",
        "        final_results.append(results.copy())\r\n",
        "    # Return a dataframe of the results\r\n",
        "    return final_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maVlDbsGvKmE",
        "outputId": "07e39b26-32e1-476a-bd4b-6d25e3e4b01d"
      },
      "source": [
        "# make a list of models\r\n",
        "models = [MultinomialNB(), DecisionTreeClassifier(), LinearSVC(), AdaBoostClassifier(), \r\n",
        "          RandomForestClassifier(), BaggingClassifier(),\r\n",
        "         LogisticRegression(), SGDClassifier(), KNeighborsClassifier(), ExtraTreesClassifier(), GradientBoostingClassifier()]\r\n",
        "\r\n",
        "\r\n",
        "re = pipeline(models, padded_train_sequences, y_train, padded_test_sequences, y_test)\r\n",
        "results = pd.DataFrame(re)\r\n",
        "results = results.reindex(columns = ['Algorithm', 'Accuracy: Test', 'Precision: Test', 'Recall: Test', 'F1 Score: Test', 'Prediction Time',\r\n",
        "                          'Accuracy: Train', 'Precision: Train', 'Recall: Train', 'F1 Score: Train', 'Training Time'])\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MultinomialNB\n",
            "Training MultinomialNB finished in 0.01 sec\n",
            "----------------------------------------------------\n",
            "Training DecisionTreeClassifier\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training DecisionTreeClassifier finished in 15.72 sec\n",
            "----------------------------------------------------\n",
            "Training LinearSVC\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training LinearSVC finished in 2.89 sec\n",
            "----------------------------------------------------\n",
            "Training AdaBoostClassifier\n",
            "Training AdaBoostClassifier finished in 2.03 sec\n",
            "----------------------------------------------------\n",
            "Training RandomForestClassifier\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training RandomForestClassifier finished in 128.31 sec\n",
            "----------------------------------------------------\n",
            "Training BaggingClassifier\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_bagging.py:645: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training BaggingClassifier finished in 107.87 sec\n",
            "----------------------------------------------------\n",
            "Training LogisticRegression\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training LogisticRegression finished in 2.51 sec\n",
            "----------------------------------------------------\n",
            "Training SGDClassifier\n",
            "Training SGDClassifier finished in 0.19 sec\n",
            "----------------------------------------------------\n",
            "Training KNeighborsClassifier\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training KNeighborsClassifier finished in 0.01 sec\n",
            "----------------------------------------------------\n",
            "Training ExtraTreesClassifier\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training ExtraTreesClassifier finished in 197.45 sec\n",
            "----------------------------------------------------\n",
            "Training GradientBoostingClassifier\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training GradientBoostingClassifier finished in 7.76 sec\n",
            "----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "ZyzIESMA2i70",
        "outputId": "c37db7b0-f68b-4c1c-fcad-eab914fe45a6"
      },
      "source": [
        "\r\n",
        "results = results.reindex(columns = ['Algorithm', 'Accuracy: Test', 'Precision: Test', 'Recall: Test', 'F1 Score: Test', 'Prediction Time',\r\n",
        "                          'Accuracy: Train', 'Precision: Train', 'Recall: Train', 'F1 Score: Train', 'Training Time'])\r\n",
        "\r\n",
        "results.sort_values(by = 'Accuracy: Test', inplace = True, ascending = False)\r\n",
        "\r\n",
        "results.reset_index(drop = True)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Algorithm</th>\n",
              "      <th>Accuracy: Test</th>\n",
              "      <th>Precision: Test</th>\n",
              "      <th>Recall: Test</th>\n",
              "      <th>F1 Score: Test</th>\n",
              "      <th>Prediction Time</th>\n",
              "      <th>Accuracy: Train</th>\n",
              "      <th>Precision: Train</th>\n",
              "      <th>Recall: Train</th>\n",
              "      <th>F1 Score: Train</th>\n",
              "      <th>Training Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ExtraTreesClassifier</td>\n",
              "      <td>0.894458</td>\n",
              "      <td>0.890049</td>\n",
              "      <td>0.926234</td>\n",
              "      <td>0.907781</td>\n",
              "      <td>11.289893</td>\n",
              "      <td>0.996511</td>\n",
              "      <td>0.996560</td>\n",
              "      <td>0.997238</td>\n",
              "      <td>0.996899</td>\n",
              "      <td>197.451664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.887431</td>\n",
              "      <td>0.876083</td>\n",
              "      <td>0.930964</td>\n",
              "      <td>0.902690</td>\n",
              "      <td>8.354014</td>\n",
              "      <td>0.996511</td>\n",
              "      <td>0.996139</td>\n",
              "      <td>0.997663</td>\n",
              "      <td>0.996900</td>\n",
              "      <td>128.313214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.886857</td>\n",
              "      <td>0.905034</td>\n",
              "      <td>0.891844</td>\n",
              "      <td>0.898390</td>\n",
              "      <td>0.004939</td>\n",
              "      <td>0.944910</td>\n",
              "      <td>0.958881</td>\n",
              "      <td>0.942459</td>\n",
              "      <td>0.950599</td>\n",
              "      <td>2.514970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BaggingClassifier</td>\n",
              "      <td>0.881265</td>\n",
              "      <td>0.875701</td>\n",
              "      <td>0.918691</td>\n",
              "      <td>0.896681</td>\n",
              "      <td>0.668664</td>\n",
              "      <td>0.987070</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>0.987081</td>\n",
              "      <td>0.988488</td>\n",
              "      <td>107.867803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>0.880691</td>\n",
              "      <td>0.882010</td>\n",
              "      <td>0.908847</td>\n",
              "      <td>0.895227</td>\n",
              "      <td>0.003598</td>\n",
              "      <td>0.979589</td>\n",
              "      <td>0.983745</td>\n",
              "      <td>0.979899</td>\n",
              "      <td>0.981818</td>\n",
              "      <td>2.894645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SGDClassifier</td>\n",
              "      <td>0.880619</td>\n",
              "      <td>0.904480</td>\n",
              "      <td>0.880082</td>\n",
              "      <td>0.892114</td>\n",
              "      <td>0.002780</td>\n",
              "      <td>0.933271</td>\n",
              "      <td>0.952678</td>\n",
              "      <td>0.927415</td>\n",
              "      <td>0.939877</td>\n",
              "      <td>0.185338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>DecisionTreeClassifier</td>\n",
              "      <td>0.877608</td>\n",
              "      <td>0.867886</td>\n",
              "      <td>0.922143</td>\n",
              "      <td>0.894192</td>\n",
              "      <td>0.076271</td>\n",
              "      <td>0.996511</td>\n",
              "      <td>0.996560</td>\n",
              "      <td>0.997238</td>\n",
              "      <td>0.996899</td>\n",
              "      <td>15.717125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MultinomialNB</td>\n",
              "      <td>0.825554</td>\n",
              "      <td>0.824131</td>\n",
              "      <td>0.875863</td>\n",
              "      <td>0.849210</td>\n",
              "      <td>0.006705</td>\n",
              "      <td>0.888005</td>\n",
              "      <td>0.892392</td>\n",
              "      <td>0.910671</td>\n",
              "      <td>0.901439</td>\n",
              "      <td>0.014950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>AdaBoostClassifier</td>\n",
              "      <td>0.823905</td>\n",
              "      <td>0.927093</td>\n",
              "      <td>0.744567</td>\n",
              "      <td>0.825865</td>\n",
              "      <td>0.526008</td>\n",
              "      <td>0.823068</td>\n",
              "      <td>0.926802</td>\n",
              "      <td>0.744167</td>\n",
              "      <td>0.825504</td>\n",
              "      <td>2.031092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>0.821969</td>\n",
              "      <td>0.924741</td>\n",
              "      <td>0.743032</td>\n",
              "      <td>0.823988</td>\n",
              "      <td>0.080037</td>\n",
              "      <td>0.825554</td>\n",
              "      <td>0.928511</td>\n",
              "      <td>0.747355</td>\n",
              "      <td>0.828142</td>\n",
              "      <td>7.757308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>KNeighborsClassifier</td>\n",
              "      <td>0.753065</td>\n",
              "      <td>0.822481</td>\n",
              "      <td>0.713756</td>\n",
              "      <td>0.764271</td>\n",
              "      <td>43.173280</td>\n",
              "      <td>0.817500</td>\n",
              "      <td>0.871535</td>\n",
              "      <td>0.792274</td>\n",
              "      <td>0.830016</td>\n",
              "      <td>0.005148</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Algorithm  Accuracy: Test  ...  F1 Score: Train  Training Time\n",
              "0         ExtraTreesClassifier        0.894458  ...         0.996899     197.451664\n",
              "1       RandomForestClassifier        0.887431  ...         0.996900     128.313214\n",
              "2           LogisticRegression        0.886857  ...         0.950599       2.514970\n",
              "3            BaggingClassifier        0.881265  ...         0.988488     107.867803\n",
              "4                    LinearSVC        0.880691  ...         0.981818       2.894645\n",
              "5                SGDClassifier        0.880619  ...         0.939877       0.185338\n",
              "6       DecisionTreeClassifier        0.877608  ...         0.996899      15.717125\n",
              "7                MultinomialNB        0.825554  ...         0.901439       0.014950\n",
              "8           AdaBoostClassifier        0.823905  ...         0.825504       2.031092\n",
              "9   GradientBoostingClassifier        0.821969  ...         0.828142       7.757308\n",
              "10        KNeighborsClassifier        0.753065  ...         0.830016       0.005148\n",
              "\n",
              "[11 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}